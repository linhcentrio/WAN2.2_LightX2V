#!/usr/bin/env python3
"""
ðŸš€ WAN2.2 Handler - ULTRA COMPACT VERSION
âœ¨ 100% features, 50% less code, maximum maintainability
"""

import runpod, os, tempfile, uuid, requests, time, torch, torch.nn.functional as F
import sys, gc, json, random, traceback, subprocess, shutil, logging, imageio
import numpy as np
from pathlib import Path
from minio import Minio
from urllib.parse import quote, urlparse
from huggingface_hub import hf_hub_download
from PIL import Image

# Quick setup
logging.basicConfig(level=logging.INFO, format='%(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
os.environ.update({'CUDA_LAUNCH_BLOCKING': '1', 'CUDA_VISIBLE_DEVICES': '0'})
sys.path.extend(['/app/ComfyUI', '/app/Practical-RIFE'])

# PyTorch + CUDA
def init_cuda():
    try:
        torch.backends.cudnn.benchmark = False
        torch.backends.cuda.matmul.allow_tf32 = False
        if torch.cuda.is_available():
            test = torch.ones(1, device='cuda').sum()
            del test; torch.cuda.empty_cache()
            logger.info(f"âœ… {torch.cuda.get_device_name(0)}")
            return True
    except: pass
    return False

CUDA_OK = init_cuda()

# ComfyUI imports
def load_nodes():
    try:
        from nodes import CLIPLoader, CLIPTextEncode, VAEDecode, VAELoader, LoadImage, ImageScale, LoraLoaderModelOnly, KSamplerAdvanced
        from custom_nodes.ComfyUI_GGUF.nodes import UnetLoaderGGUF
        from custom_nodes.ComfyUI_KJNodes.nodes.model_optimization_nodes import PathchSageAttentionKJ, WanVideoTeaCacheKJ
        from comfy_extras.nodes_wan import WanImageToVideo
        return True, {k: v for k, v in locals().items() if not k.startswith('_')}
    except Exception as e:
        logger.error(f"ComfyUI failed: {e}")
        return False, {}

NODES_OK, NODES = load_nodes()

# Storage
try:
    minio = Minio("media.aiclip.ai", "VtZ6MUPfyTOH3qSiohA2", "8boVPVIynLEKcgXirrcePxvjSk7gReIDD9pwto3t", secure=False)
except: minio = None

# Models
MODELS = {
    'high': "/app/ComfyUI/models/diffusion_models/wan2.2_i2v_high_noise_14B_Q6_K.gguf",
    'low': "/app/ComfyUI/models/diffusion_models/wan2.2_i2v_low_noise_14B_Q6_K.gguf",
    'text': "/app/ComfyUI/models/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors",
    'vae': "/app/ComfyUI/models/vae/wan_2.1_vae.safetensors",
    **{f'lx_{r}': f"/app/ComfyUI/models/loras/lightx2v_{'I2V_14B_480p_cfg_step_distill_rank32' if r=='32' else f'T2V_14B_cfg_step_distill_v2_lora_rank{r}'}_bf16.safetensors" for r in ['32','64','128']}
}

# Utils
def cleanup(): gc.collect(); torch.cuda.is_available() and torch.cuda.empty_cache()
def safe_load(loader, *args): 
    try: return loader().load(*args) if hasattr(loader(), 'load') else loader()(*args)
    except RuntimeError as e:
        if 'cuda' in str(e).lower():
            torch.backends.cudnn.enabled = False
            return loader()(*args)
        raise

# Download LoRA unified
def get_lora(url, token=None):
    if not url: return None
    ldir = "/app/ComfyUI/models/loras"; os.makedirs(ldir, exist_ok=True)
    try:
        if "huggingface.co" in url:
            parts = url.split("/")
            if len(parts) >= 7: return hf_hub_download(f"{parts[3]}/{parts[4]}", parts[-1], local_dir=ldir)
        elif "civitai.com" in url and "/models/" in url:
            mid = url.split("/models/")[1].split("?")[0].split("/")[0]
            headers = {"Authorization": f"Bearer {token}"} if token else {}
            r = requests.get(f"https://civitai.com/api/download/models/{mid}?type=Model&format=SafeTensor", 
                           headers=headers, timeout=300, stream=True)
            r.raise_for_status()
            path = f"{ldir}/civitai_{mid}_{int(time.time())}.safetensors"
            with open(path, 'wb') as f:
                for chunk in r.iter_content(8192): chunk and f.write(chunk)
            return path
        else:
            path = f"{ldir}/{os.path.basename(urlparse(url).path) or f'lora_{int(time.time())}.safetensors'}"
            r = requests.get(url, timeout=300, stream=True)
            r.raise_for_status()
            with open(path, 'wb') as f:
                for chunk in r.iter_content(8192): chunk and f.write(chunk)
            return path
    except Exception as e: logger.warning(f"LoRA failed: {e}")

# Process image compact
def proc_img(path, w=720, h=1280, mode="preserve"):
    load, scale = NODES['LoadImage'](), NODES['ImageScale']()
    img = load.load_image(path)[0]
    if img.ndim == 4: img = img[0].unsqueeze(0)
    elif img.ndim == 3: img = img.unsqueeze(0)
    _, oh, ow, _ = img.shape
    
    if mode == "stretch": return scale.upscale(img, "lanczos", w, h, "disabled")[0]
    elif mode == "crop":
        r, tr = ow/oh, w/h
        if r > tr: cw, ch, cx, cy = int(oh*tr), oh, (ow-int(oh*tr))//2, 0
        else: cw, ch, cx, cy = ow, int(ow/tr), 0, (oh-int(ow/tr))//2
        crop = img[0, cy:cy+ch, cx:cx+cw, :].unsqueeze(0)
        return scale.upscale(crop, "lanczos", w, h, "disabled")[0]
    else:  # preserve
        r, tr = ow/oh, w/h
        nw, nh = (w, int(w/r)) if r > tr else (int(h*r), h)
        nw, nh = (nw//8)*8, (nh//8)*8
        scaled = scale.upscale(img, "lanczos", nw, nh, "disabled")[0]
        if nw != w or nh != h:
            px, py = (w-nw)//2, (h-nh)//2
            if scaled.ndim == 3: scaled = scaled.unsqueeze(0)
            try:
                pad = F.pad(scaled.permute(0,3,1,2), (px, w-nw-px, py, h-nh-py), value=0)
                return pad.permute(0,2,3,1)[0]
            except: return scale.upscale(img, "lanczos", w, h, "disabled")[0]
        return scaled

# RIFE compact
def rife(vpath, factor=2):
    if not os.path.exists("/app/Practical-RIFE/inference_video.py"): return vpath
    odir = "/app/rife_output"; os.makedirs(odir, exist_ok=True)
    base = os.path.splitext(os.path.basename(vpath))[0]
    out = f"{odir}/{base}_rife_{factor}x.mp4"
    rinput = f"/app/Practical-RIFE/{os.path.basename(vpath)}"
    shutil.copy2(vpath, rinput)
    
    try:
        cwd = os.getcwd(); os.chdir("/app/Practical-RIFE")
        cmd = ["python3", "inference_video.py", f"--multi={factor}", f"--video={os.path.basename(vpath)}", "--scale=1.0", "--fps=30"]
        subprocess.run(cmd, capture_output=True, timeout=600)
        
        for pat in [f"{base}_{factor}X.mp4", f"{base}_interpolated.mp4", "output.mp4"]:
            if os.path.exists(pat):
                shutil.move(pat, out)
                logger.info(f"âœ… RIFE {factor}x done")
                return out
        return vpath
    except: return vpath
    finally: os.chdir(cwd); os.path.exists(rinput) and os.remove(rinput)

# Save video compact
def save_vid(frames, path, fps=16):
    if torch.is_tensor(frames): frames = frames.detach().cpu().float().numpy()
    if frames.ndim == 5: frames = frames[0]
    frames = (frames * 255).astype(np.uint8) if frames.max() <= 1.0 else np.clip(frames, 0, 255).astype(np.uint8)
    if frames.shape[-1] == 1: frames = np.repeat(frames, 3, -1)
    elif frames.shape[-1] == 4: frames = frames[:,:,:,:3]
    
    os.makedirs(os.path.dirname(path), exist_ok=True)
    for params in [{'fps': fps, 'codec': 'libx264', 'pixelformat': 'yuv420p'}, {'fps': fps}]:
        try:
            with imageio.get_writer(path, **params) as w:
                for f in frames: w.append_data(f)
            if os.path.exists(path) and os.path.getsize(path) > 0:
                logger.info(f"âœ… Video: {os.path.getsize(path)/1024/1024:.1f}MB")
                return path
        except: continue
    raise RuntimeError("Video save failed")

# Main generation - ultra compact
def generate(img_path, **p):
    logger.info("ðŸŽ¬ WAN2.2 Generation starting...")
    pos, neg = p.get('positive_prompt', ''), p.get('negative_prompt', 'è‰²è°ƒè‰³ä¸½ï¼Œè¿‡æ›ï¼Œé™æ€')
    w, h, seed = p.get('width', 720), p.get('height', 1280), p.get('seed', 0) or random.randint(1, 2**32-1)
    steps, hsteps, cfg = p.get('steps', 6), p.get('high_noise_steps', 3), p.get('cfg_scale', 1.0)
    frames, fps = p.get('frames', 65), p.get('fps', 16)
    
    with torch.inference_mode():
        # Nodes
        loader, vloader, sampler, decoder, i2v = (NODES[k]() for k in ['UnetLoaderGGUF', 'VAELoader', 'KSamplerAdvanced', 'VAEDecode', 'WanImageToVideo'])
        lora_load = NODES['LoraLoaderModelOnly']()
        
        # Text encode
        clip = safe_load(NODES['CLIPLoader'], "umt5_xxl_fp8_e4m3fn_scaled.safetensors", "wan", "default")[0]
        enc = NODES['CLIPTextEncode']()
        pos_cond, neg_cond = enc.encode(clip, pos)[0], enc.encode(clip, neg)[0]
        del clip; cleanup()
        
        # Process image
        img = proc_img(img_path, w, h, p.get('aspect_mode', 'preserve'))
        
        # VAE + I2V
        vae = vloader.load_vae("wan_2.1_vae.safetensors")[0]
        pos_out, neg_out, latent = i2v.encode(pos_cond, neg_cond, vae, w, h, frames, 1, img, None)
        
        # Download LoRAs
        loras = [(p.get('lora_url'), p.get('lora_strength', 1.0))] if p.get('use_lora') else []
        lora_files = [get_lora(url, p.get('civitai_token')) for url, _ in loras if url]
        
        # Stage 1: High noise
        logger.info("ðŸŽ¯ Stage 1...")
        model = loader.load_unet("wan2.2_i2v_high_noise_14B_Q6_K.gguf")[0]
        
        # Apply LoRAs
        for lfile, (_, strength) in zip(lora_files, loras):
            if lfile: model = lora_load.load_lora_model_only(model, os.path.basename(lfile), strength)[0]
        
        # LightX2V
        if p.get('use_lightx2v'):
            lx_file = MODELS.get(f"lx_{p.get('lightx2v_rank', '32')}")
            if lx_file and os.path.exists(lx_file):
                model = lora_load.load_lora_model_only(model, os.path.basename(lx_file), p.get('lightx2v_strength', 3.0))[0]
        
        sampled = sampler.sample(model, "enable", seed, steps, cfg, "euler", "simple", 
                               pos_out, neg_out, latent, 0, hsteps, "enable")[0]
        del model; cleanup()
        
        # Stage 2: Low noise
        logger.info("ðŸŽ¯ Stage 2...")
        model = loader.load_unet("wan2.2_i2v_low_noise_14B_Q6_K.gguf")[0]
        
        # Reapply LoRAs
        for lfile, (_, strength) in zip(lora_files, loras):
            if lfile: model = lora_load.load_lora_model_only(model, os.path.basename(lfile), strength)[0]
        if p.get('use_lightx2v') and lx_file and os.path.exists(lx_file):
            model = lora_load.load_lora_model_only(model, os.path.basename(lx_file), p.get('lightx2v_strength', 3.0))[0]
        
        sampled = sampler.sample(model, "disable", seed, steps, cfg, "euler", "simple",
                               pos_out, neg_out, sampled, hsteps, 10000, "disable")[0]
        del model; cleanup()
        
        # Decode & save
        logger.info("ðŸŽ¨ Decoding...")
        decoded = decoder.decode(vae, sampled)[0]
        del vae; cleanup()
        
        out_path = f"/app/ComfyUI/output/wan22_ultra_{uuid.uuid4().hex[:8]}.mp4"
        final_path = save_vid(decoded, out_path, fps)
        
        # RIFE
        if p.get('enable_interpolation') and p.get('interpolation_factor', 2) > 1:
            logger.info("ðŸ”„ RIFE...")
            final_path = rife(final_path, p.get('interpolation_factor', 2))
        
        logger.info("âœ… Done!")
        return final_path

# Validation compact
def validate(p):
    req = ["image_url", "positive_prompt"]
    for r in req:
        if not p.get(r): return False, f"Missing: {r}"
    
    # Normalize
    assist = str(p.get('prompt_assist', 'none')).lower().strip()
    assist_map = {'none':'none', 'walking to viewers':'walking to viewers', 'walking_to_viewers':'walking to viewers',
                  'walking from behind':'walking from behind', 'walking_from_behind':'walking from behind',
                  'dance':'b3ll13-d8nc3r', 'dancing':'b3ll13-d8nc3r', 'b3ll13-d8nc3r':'b3ll13-d8nc3r'}
    if assist not in assist_map: return False, "Invalid prompt_assist"
    p['prompt_assist'] = assist_map[assist]
    
    # Ranges
    w, h = p.get('width', 720), p.get('height', 1280)
    if not (256 <= w <= 1536 and 256 <= h <= 1536): return False, "Invalid dimensions"
    if not (1 <= p.get('frames', 65) <= 150): return False, "Invalid frames"
    return True, "OK"

# Upload
def upload(path, name):
    if not minio: raise RuntimeError("Storage unavailable")
    minio.fput_object("video", name, path)
    return f"https://media.aiclip.ai/video/{quote(name)}"

# Handler - ultra compact
def handler(job):
    jid, start = job.get("id", "unknown"), time.time()
    try:
        p = job.get("input", {})
        ok, msg = validate(p)
        if not ok: return {"error": msg, "status": "failed"}
        if not NODES_OK: return {"error": "System unavailable", "status": "failed"}
        
        logger.info(f"ðŸš€ Job {jid}")
        
        with tempfile.TemporaryDirectory() as tmp:
            # Download image
            img_path = f"{tmp}/input.jpg"
            r = requests.get(p["image_url"], timeout=60, stream=True)
            r.raise_for_status()
            with open(img_path, 'wb') as f:
                for chunk in r.iter_content(8192): chunk and f.write(chunk)
            
            # Generate
            gen_start = time.time()
            out_path = generate(img_path, **p)
            gen_time = time.time() - gen_start
            
            if not out_path: return {"error": "Generation failed"}
            
            # Upload
            fname = f"wan22_ultra_{jid}_{uuid.uuid4().hex[:8]}.mp4"
            url = upload(out_path, fname)
            
            # Stats
            total = time.time() - start
            size = os.path.getsize(out_path) / 1024 / 1024
            duration = p.get('frames', 65) / p.get('fps', 16)
            
            logger.info(f"âœ… {jid}: {total:.1f}s")
            
            return {
                "output_video_url": url,
                "processing_time_seconds": round(total, 2),
                "generation_time_seconds": round(gen_time, 2),
                "video_info": {
                    "width": p.get('width', 720), "height": p.get('height', 1280),
                    "frames": p.get('frames', 65), "fps": p.get('fps', 16),
                    "duration_seconds": round(duration, 2), "file_size_mb": round(size, 2),
                    "interpolated": "_rife_" in os.path.basename(out_path)
                },
                "status": "completed"
            }
    
    except Exception as e:
        logger.error(f"âŒ {jid}: {e}")
        return {"error": str(e), "status": "failed", "processing_time_seconds": round(time.time()-start, 2)}
    finally: cleanup()

# Main
if __name__ == "__main__":
    logger.info("ðŸš€ WAN2.2 ULTRA COMPACT ready!")
    # Health check
    issues = []
    if not CUDA_OK: issues.append("CUDA")
    if not NODES_OK: issues.append("ComfyUI") 
    if not minio: issues.append("Storage")
    missing = [k for k,v in MODELS.items() if not os.path.exists(v)]
    if missing: issues.append(f"Models: {missing}")
    
    logger.info(f"Status: {'âš ï¸ ' + ', '.join(issues) if issues else 'âœ… All ready'}")
    logger.info("Features: CUDA Safety, Aspect Control, RIFE, LoRA, LightX2V")
    
    runpod.serverless.start({"handler": handler})
