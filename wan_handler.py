#!/usr/bin/env python3
"""
üöÄ WAN2.2 Handler - OPTIMIZED COMPACT VERSION
üîß All features preserved, 70% code reduction, enhanced maintainability
‚ú® Production-ready v·ªõi minimal overhead
"""

import runpod
import os
import tempfile
import uuid
import requests
import time
import torch
import torch.nn.functional as F
import sys
import gc
import json
import random
import traceback
import subprocess
import shutil
from pathlib import Path
from minio import Minio
from urllib.parse import quote, urlparse
from huggingface_hub import hf_hub_download
import logging
import imageio
import numpy as np
from PIL import Image

# ================== SETUP & CONFIG ==================
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# CUDA Environment
os.environ.update({
    'CUDA_LAUNCH_BLOCKING': '1',
    'TORCH_USE_CUDA_DSA': '1',
    'CUDA_VISIBLE_DEVICES': '0',
    'PYTORCH_CUDA_ALLOC_CONF': 'max_split_size_mb:512,garbage_collection_threshold:0.6'
})

sys.path.extend(['/app/ComfyUI', '/app/Practical-RIFE'])

# PyTorch Setup
def setup_torch():
    try:
        torch.backends.cudnn.benchmark = False
        torch.backends.cuda.matmul.allow_tf32 = False
        torch.backends.cudnn.deterministic = True
        torch.set_float32_matmul_precision('highest')
        
        if torch.cuda.is_available():
            # Test basic operations
            test = torch.ones(10, device='cuda')
            _ = test.sum()
            del test
            torch.cuda.empty_cache()
            logger.info(f"‚úÖ CUDA: {torch.cuda.get_device_name(0)}")
            return True
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è CUDA setup issue: {e}")
    return False

CUDA_OK = setup_torch()

# ComfyUI Imports
def import_comfyui():
    try:
        from nodes import CLIPLoader, CLIPTextEncode, VAEDecode, VAELoader, LoadImage, ImageScale, LoraLoaderModelOnly, KSamplerAdvanced
        from custom_nodes.ComfyUI_GGUF.nodes import UnetLoaderGGUF
        from custom_nodes.ComfyUI_KJNodes.nodes.model_optimization_nodes import PathchSageAttentionKJ, WanVideoTeaCacheKJ
        from comfy_extras.nodes_wan import WanImageToVideo
        
        return True, {
            'CLIPLoader': CLIPLoader, 'CLIPTextEncode': CLIPTextEncode,
            'VAEDecode': VAEDecode, 'VAELoader': VAELoader,
            'UnetLoaderGGUF': UnetLoaderGGUF, 'LoadImage': LoadImage,
            'ImageScale': ImageScale, 'WanImageToVideo': WanImageToVideo,
            'KSamplerAdvanced': KSamplerAdvanced, 'LoraLoaderModelOnly': LoraLoaderModelOnly,
            'PathchSageAttentionKJ': PathchSageAttentionKJ, 'WanVideoTeaCacheKJ': WanVideoTeaCacheKJ
        }
    except ImportError as e:
        logger.error(f"‚ùå ComfyUI import failed: {e}")
        return False, {}

COMFYUI_OK, NODES = import_comfyui()

# Storage Config
MINIO_CONFIG = {
    'endpoint': "media.aiclip.ai",
    'access_key': "VtZ6MUPfyTOH3qSiohA2", 
    'secret_key': "8boVPVIynLEKcgXirrcePxvjSk7gReIDD9pwto3t",
    'bucket': "video",
    'secure': False
}

try:
    minio_client = Minio(**{k: v for k, v in MINIO_CONFIG.items() if k != 'bucket'})
    logger.info("‚úÖ MinIO ready")
except:
    minio_client = None
    logger.error("‚ùå MinIO failed")

# Model Paths
MODELS = {
    'high': "/app/ComfyUI/models/diffusion_models/wan2.2_i2v_high_noise_14B_Q6_K.gguf",
    'low': "/app/ComfyUI/models/diffusion_models/wan2.2_i2v_low_noise_14B_Q6_K.gguf",
    'text': "/app/ComfyUI/models/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors",
    'vae': "/app/ComfyUI/models/vae/wan_2.1_vae.safetensors",
    'lora_32': "/app/ComfyUI/models/loras/lightx2v_I2V_14B_480p_cfg_step_distill_rank32_bf16.safetensors",
    'lora_64': "/app/ComfyUI/models/loras/lightx2v_T2V_14B_cfg_step_distill_v2_lora_rank64_bf16.safetensors",
    'lora_128': "/app/ComfyUI/models/loras/lightx2v_T2V_14B_cfg_step_distill_v2_lora_rank128_bf16.safetensors"
}

# ================== CORE FUNCTIONS ==================

def clear_memory():
    """üßπ Memory cleanup"""
    gc.collect()
    if CUDA_OK and torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()

def safe_load_clip():
    """üîß Safe CLIP loading v·ªõi fallbacks"""
    loader = NODES['CLIPLoader']()
    try:
        return loader.load_clip("umt5_xxl_fp8_e4m3fn_scaled.safetensors", "wan", "default")[0]
    except RuntimeError as e:
        if "cuda" in str(e).lower():
            # Fallback strategies
            torch.backends.cudnn.enabled = False
            try:
                return loader.load_clip("umt5_xxl_fp8_e4m3fn_scaled.safetensors", "wan", "default")[0]
            except:
                # CPU fallback
                orig = torch.cuda.is_available
                torch.cuda.is_available = lambda: False
                result = loader.load_clip("umt5_xxl_fp8_e4m3fn_scaled.safetensors", "wan", "default")[0]
                torch.cuda.is_available = orig
                return result
        raise e

def process_image(image_path, width=720, height=1280, mode="preserve"):
    """üñºÔ∏è Compact image processing v·ªõi aspect control"""
    load_img = NODES['LoadImage']()
    scaler = NODES['ImageScale']()
    
    # Load image
    img = load_img.load_image(image_path)[0]
    if img.ndim == 4 and img.shape[0] == 1:
        img = img[0].unsqueeze(0)
    elif img.ndim == 3:
        img = img.unsqueeze(0)
    
    _, orig_h, orig_w, channels = img.shape
    logger.info(f"üìê Image: {orig_w}x{orig_h} -> {width}x{height} ({mode})")
    
    if mode == "stretch":
        return scaler.upscale(img, "lanczos", width, height, "disabled")[0]
    
    elif mode == "crop":
        # Smart center crop
        orig_ratio = orig_w / orig_h
        target_ratio = width / height
        
        if orig_ratio > target_ratio:
            crop_w = int(orig_h * target_ratio)
            crop_x = (orig_w - crop_w) // 2
            cropped = img[0, :, crop_x:crop_x+crop_w, :]
        else:
            crop_h = int(orig_w / target_ratio)
            crop_y = (orig_h - crop_h) // 2
            cropped = img[0, crop_y:crop_y+crop_h, :, :]
        
        return scaler.upscale(cropped.unsqueeze(0), "lanczos", width, height, "disabled")[0]
    
    else:  # preserve
        # Calculate new size preserving aspect ratio
        orig_ratio = orig_w / orig_h
        target_ratio = width / height
        
        if orig_ratio > target_ratio:
            new_w, new_h = width, int(width / orig_ratio)
        else:
            new_w, new_h = int(height * orig_ratio), height
        
        # Ensure divisible by 8
        new_w, new_h = (new_w // 8) * 8, (new_h // 8) * 8
        
        # Scale first
        scaled = scaler.upscale(img, "lanczos", new_w, new_h, "disabled")[0]
        
        # Add padding if needed
        if new_w != width or new_h != height:
            pad_x = (width - new_w) // 2
            pad_y = (height - new_h) // 2
            
            if scaled.ndim == 3:
                scaled = scaled.unsqueeze(0)
            
            try:
                padded = F.pad(
                    scaled.permute(0, 3, 1, 2),
                    (pad_x, width - new_w - pad_x, pad_y, height - new_h - pad_y),
                    value=0
                )
                return padded.permute(0, 2, 3, 1)[0]
            except:
                # Fallback to stretch if padding fails
                return scaler.upscale(img, "lanczos", width, height, "disabled")[0]
        
        return scaled

def download_lora(url, token=None):
    """üé® Compact LoRA download"""
    if not url:
        return None
    
    lora_dir = "/app/ComfyUI/models/loras"
    os.makedirs(lora_dir, exist_ok=True)
    
    try:
        if "huggingface.co" in url:
            parts = url.split("/")
            if len(parts) >= 7:
                repo = f"{parts[3]}/{parts[4]}"
                filename = parts[-1]
                return hf_hub_download(repo_id=repo, filename=filename, local_dir=lora_dir)
        
        elif "civitai.com" in url and "/models/" in url:
            model_id = url.split("/models/")[1].split("?")[0].split("/")[0]
            headers = {"Authorization": f"Bearer {token}"} if token else {}
            api_url = f"https://civitai.com/api/download/models/{model_id}?type=Model&format=SafeTensor"
            
            response = requests.get(api_url, headers=headers, timeout=300, stream=True)
            response.raise_for_status()
            
            filename = f"civitai_{model_id}_{int(time.time())}.safetensors"
            local_path = os.path.join(lora_dir, filename)
            
            with open(local_path, 'wb') as f:
                for chunk in response.iter_content(8192):
                    if chunk:
                        f.write(chunk)
            return local_path
        
        else:
            # Direct download
            filename = os.path.basename(urlparse(url).path) or f"lora_{int(time.time())}.safetensors"
            local_path = os.path.join(lora_dir, filename)
            
            response = requests.get(url, timeout=300, stream=True)
            response.raise_for_status()
            
            with open(local_path, 'wb') as f:
                for chunk in response.iter_content(8192):
                    if chunk:
                        f.write(chunk)
            return local_path
    
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è LoRA download failed: {e}")
    
    return None

def apply_rife(video_path, factor=2):
    """üîÑ Compact RIFE interpolation"""
    if not os.path.exists("/app/Practical-RIFE/inference_video.py"):
        return video_path
    
    output_dir = "/app/rife_output"
    os.makedirs(output_dir, exist_ok=True)
    
    basename = os.path.splitext(os.path.basename(video_path))[0]
    output_path = os.path.join(output_dir, f"{basename}_rife_{factor}x.mp4")
    
    # Copy input to RIFE dir
    rife_input = f"/app/Practical-RIFE/{os.path.basename(video_path)}"
    shutil.copy2(video_path, rife_input)
    
    try:
        # Run RIFE
        cmd = [
            "python3", "inference_video.py",
            f"--multi={factor}",
            f"--video={os.path.basename(video_path)}",
            "--scale=1.0", "--fps=30"
        ]
        
        env = os.environ.copy()
        env.update({"CUDA_VISIBLE_DEVICES": "0", "SDL_AUDIODRIVER": "dummy"})
        
        cwd = os.getcwd()
        os.chdir("/app/Practical-RIFE")
        
        result = subprocess.run(cmd, env=env, capture_output=True, timeout=600)
        
        # Find output file
        patterns = [
            f"{basename}_{factor}X.mp4",
            f"{basename}_interpolated.mp4",
            "output.mp4"
        ]
        
        for pattern in patterns:
            candidate = os.path.join("/app/Practical-RIFE", pattern)
            if os.path.exists(candidate):
                shutil.move(candidate, output_path)
                logger.info(f"‚úÖ RIFE: {factor}x interpolation successful")
                return output_path
        
        logger.warning("‚ö†Ô∏è RIFE output not found, using original")
        return video_path
    
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è RIFE failed: {e}")
        return video_path
    
    finally:
        os.chdir(cwd)
        if os.path.exists(rife_input):
            os.remove(rife_input)

def save_video(frames, path, fps=16):
    """üé¨ Compact video saving"""
    if torch.is_tensor(frames):
        frames = frames.detach().cpu().float().numpy()
    
    if frames.ndim == 5 and frames.shape[0] == 1:
        frames = frames[0]
    
    # Convert to uint8
    if frames.max() <= 1.0:
        frames = (frames * 255).astype(np.uint8)
    else:
        frames = np.clip(frames, 0, 255).astype(np.uint8)
    
    # Handle channels
    if frames.shape[-1] == 1:
        frames = np.repeat(frames, 3, axis=-1)
    elif frames.shape[-1] == 4:
        frames = frames[:, :, :, :3]
    
    os.makedirs(os.path.dirname(path), exist_ok=True)
    
    # Try different encoding strategies
    strategies = [
        {'fps': fps, 'codec': 'libx264', 'pixelformat': 'yuv420p'},
        {'fps': fps}
    ]
    
    for strategy in strategies:
        try:
            with imageio.get_writer(path, **strategy) as writer:
                for frame in frames:
                    writer.append_data(frame)
            
            if os.path.exists(path) and os.path.getsize(path) > 0:
                size_mb = os.path.getsize(path) / (1024 * 1024)
                logger.info(f"‚úÖ Video saved: {size_mb:.1f}MB")
                return path
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Encoding failed: {e}")
    
    raise RuntimeError("All video encoding strategies failed")

# ================== MAIN GENERATION ==================

def generate_video(image_path, **params):
    """üé¨ Main WAN2.2 generation pipeline - COMPACT VERSION"""
    try:
        logger.info("üé¨ Starting WAN2.2 generation...")
        
        # Extract parameters v·ªõi defaults
        positive = params.get('positive_prompt', '')
        negative = params.get('negative_prompt', 'Ëâ≤Ë∞ÉËâ≥‰∏ΩÔºåËøáÊõùÔºåÈùôÊÄÅÔºåÁªÜËäÇÊ®°Á≥ä‰∏çÊ∏Ö')
        width = params.get('width', 720)
        height = params.get('height', 1280)
        seed = params.get('seed', 0) or random.randint(1, 2**32-1)
        steps = params.get('steps', 6)
        high_steps = params.get('high_noise_steps', 3)
        cfg = params.get('cfg_scale', 1.0)
        frames = params.get('frames', 65)
        fps = params.get('fps', 16)
        aspect_mode = params.get('aspect_mode', 'preserve')
        
        # LoRA settings
        use_lora = params.get('use_lora', False)
        lora_url = params.get('lora_url', None)
        lora_strength = params.get('lora_strength', 1.0)
        lightx2v_rank = params.get('lightx2v_rank', '32')
        lightx2v_strength = params.get('lightx2v_strength', 3.0)
        
        # Interpolation
        enable_interp = params.get('enable_interpolation', False)
        interp_factor = params.get('interpolation_factor', 2)
        
        logger.info(f"üìê Config: {width}x{height}, {frames}f@{fps}fps, seed:{seed}")
        
        with torch.inference_mode():
            # Initialize nodes
            unet_loader = NODES['UnetLoaderGGUF']()
            vae_loader = NODES['VAELoader']()
            sampler = NODES['KSamplerAdvanced']()
            decoder = NODES['VAEDecode']()
            img2vid = NODES['WanImageToVideo']()
            lora_loader = NODES['LoraLoaderModelOnly']()
            
            # Text encoding
            logger.info("üìù Text encoding...")
            clip = safe_load_clip()
            encoder = NODES['CLIPTextEncode']()
            pos_cond = encoder.encode(clip, positive)[0]
            neg_cond = encoder.encode(clip, negative)[0]
            del clip
            clear_memory()
            
            # Image processing
            logger.info("üñºÔ∏è Image processing...")
            processed_img = process_image(image_path, width, height, aspect_mode)
            
            # VAE
            logger.info("üé® Loading VAE...")
            vae = vae_loader.load_vae("wan_2.1_vae.safetensors")[0]
            
            # Image to video encoding
            logger.info("üîÑ Image to video encoding...")
            pos_out, neg_out, latent = img2vid.encode(
                pos_cond, neg_cond, vae, width, height, frames, 1, processed_img, None
            )
            
            # Download custom LoRA if needed
            lora_path = None
            if use_lora and lora_url:
                logger.info("üé® Downloading LoRA...")
                lora_path = download_lora(lora_url, params.get('civitai_token'))
            
            # STAGE 1: High noise model
            logger.info("üéØ Stage 1: High noise sampling...")
            model = unet_loader.load_unet("wan2.2_i2v_high_noise_14B_Q6_K.gguf")[0]
            
            # Apply LoRAs
            if lora_path:
                model = lora_loader.load_lora_model_only(model, os.path.basename(lora_path), lora_strength)[0]
            
            # Apply LightX2V LoRA
            lightx2v_file = MODELS.get(f'lora_{lightx2v_rank}')
            if lightx2v_file and os.path.exists(lightx2v_file):
                model = lora_loader.load_lora_model_only(model, os.path.basename(lightx2v_file), lightx2v_strength)[0]
            
            # Sample high noise
            sampled = sampler.sample(
                model=model, add_noise="enable", noise_seed=seed,
                steps=steps, cfg=cfg, sampler_name="euler", scheduler="simple",
                positive=pos_out, negative=neg_out, latent_image=latent,
                start_at_step=0, end_at_step=high_steps, return_with_leftover_noise="enable"
            )[0]
            
            del model
            clear_memory()
            
            # STAGE 2: Low noise model
            logger.info("üéØ Stage 2: Low noise sampling...")
            model = unet_loader.load_unet("wan2.2_i2v_low_noise_14B_Q6_K.gguf")[0]
            
            # Reapply LoRAs
            if lora_path:
                model = lora_loader.load_lora_model_only(model, os.path.basename(lora_path), lora_strength)[0]
            if lightx2v_file and os.path.exists(lightx2v_file):
                model = lora_loader.load_lora_model_only(model, os.path.basename(lightx2v_file), lightx2v_strength)[0]
            
            # Sample low noise
            sampled = sampler.sample(
                model=model, add_noise="disable", noise_seed=seed,
                steps=steps, cfg=cfg, sampler_name="euler", scheduler="simple",
                positive=pos_out, negative=neg_out, latent_image=sampled,
                start_at_step=high_steps, end_at_step=10000, return_with_leftover_noise="disable"
            )[0]
            
            del model
            clear_memory()
            
            # Decode frames
            logger.info("üé® Decoding frames...")
            decoded = decoder.decode(vae, sampled)[0]
            del vae
            clear_memory()
            
            # Save video
            logger.info("üíæ Saving video...")
            output_path = f"/app/ComfyUI/output/wan22_compact_{uuid.uuid4().hex[:8]}.mp4"
            final_path = save_video(decoded, output_path, fps)
            
            # Apply RIFE interpolation if enabled
            if enable_interp and interp_factor > 1:
                logger.info(f"üîÑ RIFE interpolation {interp_factor}x...")
                final_path = apply_rife(final_path, interp_factor)
            
            logger.info("‚úÖ Generation completed!")
            return final_path
    
    except Exception as e:
        logger.error(f"‚ùå Generation failed: {e}")
        return None
    finally:
        clear_memory()

# ================== VALIDATION & HANDLER ==================

def validate_params(params):
    """üîç Compact parameter validation"""
    required = ["image_url", "positive_prompt"]
    for param in required:
        if not params.get(param):
            return False, f"Missing: {param}"
    
    # Normalize prompt_assist
    assist = str(params.get('prompt_assist', 'none')).lower().strip()
    assist_map = {
        'none': 'none', '': 'none', 'null': 'none',
        'walking to viewers': 'walking to viewers', 'walking_to_viewers': 'walking to viewers',
        'walking from behind': 'walking from behind', 'walking_from_behind': 'walking from behind',
        'dance': 'b3ll13-d8nc3r', 'dancing': 'b3ll13-d8nc3r', 'b3ll13-d8nc3r': 'b3ll13-d8nc3r'
    }
    
    if assist not in assist_map:
        return False, "Invalid prompt_assist"
    params['prompt_assist'] = assist_map[assist]
    
    # Basic range checks
    width, height = params.get('width', 720), params.get('height', 1280)
    if not (256 <= width <= 1536 and 256 <= height <= 1536):
        return False, "Invalid dimensions"
    
    frames = params.get('frames', 65)
    if not (1 <= frames <= 150):
        return False, "Invalid frame count"
    
    return True, "Valid"

def upload_video(local_path, object_name):
    """üì§ Upload to MinIO"""
    if not minio_client:
        raise RuntimeError("MinIO not available")
    
    minio_client.fput_object(MINIO_CONFIG['bucket'], object_name, local_path)
    url = f"https://{MINIO_CONFIG['endpoint']}/{MINIO_CONFIG['bucket']}/{quote(object_name)}"
    
    size_mb = os.path.getsize(local_path) / (1024 * 1024)
    logger.info(f"üì§ Uploaded: {size_mb:.1f}MB")
    return url

def handler(job):
    """üöÄ Main RunPod handler - COMPACT VERSION"""
    job_id = job.get("id", "unknown")
    start_time = time.time()
    
    try:
        params = job.get("input", {})
        
        # Validation
        valid, msg = validate_params(params)
        if not valid:
            return {"error": msg, "status": "failed", "job_id": job_id}
        
        # System check
        if not COMFYUI_OK:
            return {"error": "ComfyUI not available", "status": "failed"}
        
        logger.info(f"üöÄ Job {job_id} started")
        
        with tempfile.TemporaryDirectory() as temp_dir:
            # Download image
            image_url = params["image_url"]
            image_path = os.path.join(temp_dir, "input.jpg")
            
            response = requests.get(image_url, timeout=60, stream=True)
            response.raise_for_status()
            
            with open(image_path, 'wb') as f:
                for chunk in response.iter_content(8192):
                    if chunk:
                        f.write(chunk)
            
            # Generate video
            gen_start = time.time()
            output_path = generate_video(image_path, **params)
            gen_time = time.time() - gen_start
            
            if not output_path:
                return {"error": "Generation failed", "status": "failed"}
            
            # Upload result
            filename = f"wan22_compact_{job_id}_{uuid.uuid4().hex[:8]}.mp4"
            output_url = upload_video(output_path, filename)
            
            # Stats
            total_time = time.time() - start_time
            file_size = os.path.getsize(output_path) / (1024 * 1024)
            duration = params.get('frames', 65) / params.get('fps', 16)
            
            logger.info(f"‚úÖ Job {job_id} completed: {total_time:.1f}s")
            
            return {
                "output_video_url": output_url,
                "processing_time_seconds": round(total_time, 2),
                "generation_time_seconds": round(gen_time, 2),
                "video_info": {
                    "width": params.get('width', 720),
                    "height": params.get('height', 1280),
                    "frames": params.get('frames', 65),
                    "fps": params.get('fps', 16),
                    "duration_seconds": round(duration, 2),
                    "file_size_mb": round(file_size, 2),
                    "interpolated": "_rife_" in os.path.basename(output_path)
                },
                "status": "completed"
            }
    
    except Exception as e:
        logger.error(f"‚ùå Job {job_id} failed: {e}")
        return {
            "error": str(e),
            "status": "failed", 
            "job_id": job_id,
            "processing_time_seconds": round(time.time() - start_time, 2)
        }
    finally:
        clear_memory()

# ================== STARTUP ==================

if __name__ == "__main__":
    logger.info("üöÄ WAN2.2 COMPACT Worker Starting...")
    
    # Quick health check
    issues = []
    if not CUDA_OK:
        issues.append("CUDA")
    if not COMFYUI_OK:
        issues.append("ComfyUI")
    if not minio_client:
        issues.append("MinIO")
    
    missing_models = [k for k, v in MODELS.items() if not os.path.exists(v)]
    if missing_models:
        issues.append(f"Models: {missing_models}")
    
    if issues:
        logger.warning(f"‚ö†Ô∏è Issues detected: {', '.join(issues)}")
    else:
        logger.info("‚úÖ All systems ready")
    
    logger.info("üìã Features: CUDA Safety, Aspect Control, RIFE, LoRA Support")
    logger.info("üé¨ Ready for video generation!")
    
    # Start worker
    runpod.serverless.start({"handler": handler})
