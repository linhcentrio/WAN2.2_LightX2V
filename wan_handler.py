#!/usr/bin/env python3
"""
ğŸš€ RunPod Serverless Handler cho WAN2.2 LightX2V Q6_K - PRODUCTION FINAL VERSION
ğŸ”§ All Fixes Applied: CUDA Compatibility, Tensor Shapes, Aspect Ratios, RIFE Interpolation
âœ¨ Production Ready vá»›i comprehensive error handling vÃ  optimization
ğŸ“‹ Supports multiple image sizes vÃ  aspect ratios má»™t cÃ¡ch chÃ­nh xÃ¡c
"""

import runpod
import os
import tempfile
import uuid
import requests
import time
import torch
import torch.nn.functional as F
from torch.cuda.amp import autocast
import sys
import gc
import json
import random
import traceback
import subprocess
import shutil
from pathlib import Path
from minio import Minio
from urllib.parse import quote, urlparse
from huggingface_hub import hf_hub_download
import logging
import imageio
import numpy as np
from PIL import Image

# ================== ENHANCED CUDA COMPATIBILITY SETUP ==================
# ğŸ›¡ï¸ Comprehensive CUDA compatibility fixes
os.environ.update({
    'CUDA_LAUNCH_BLOCKING': '1',
    'TORCH_USE_CUDA_DSA': '1',
    'CUDA_VISIBLE_DEVICES': '0',
    'PYTORCH_CUDA_ALLOC_CONF': 'max_split_size_mb:512,garbage_collection_threshold:0.6,expandable_segments:True',
    'PYTORCH_KERNEL_CACHE_PATH': '/tmp/pytorch_kernel_cache',
    'CUDA_MODULE_LOADING': 'LAZY',
    'CUBLAS_WORKSPACE_CONFIG': ':4096:8',
    'PYTORCH_CUDA_ALLOC_SYNC_MEMORY': '1',
    'CUDA_DEVICE_ORDER': 'PCI_BUS_ID'
})

# Configure comprehensive logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Add system paths
sys.path.insert(0, '/app/ComfyUI')
sys.path.insert(0, '/app/Practical-RIFE')

# ================== CUDA-SAFE PYTORCH CONFIGURATION ==================
def setup_pytorch_production():
    """ğŸ”§ Production-grade PyTorch setup vá»›i comprehensive compatibility"""
    try:
        if torch.cuda.is_available():
            # ğŸ›¡ï¸ Conservative CUDA settings cho maximum compatibility
            torch.backends.cudnn.benchmark = False  # Disable auto-tuning
            torch.backends.cuda.matmul.allow_tf32 = False  # Disable TF32
            torch.backends.cudnn.allow_tf32 = False
            torch.backends.cudnn.deterministic = True  # Ensure reproducibility
            torch.set_float32_matmul_precision('highest')  # Most stable precision
            
            # Disable problematic optimizations
            torch.backends.cuda.enable_flash_sdp(False)
            torch.backends.cuda.enable_mem_efficient_sdp(False)
            torch.backends.cuda.enable_math_sdp(True)
            
            # Test CUDA functionality
            try:
                device = torch.cuda.current_device()
                device_name = torch.cuda.get_device_name(device)
                compute_cap = torch.cuda.get_device_capability(device)
                memory_gb = torch.cuda.get_device_properties(device).total_memory / 1e9
                
                logger.info(f"ğŸ¯ GPU: {device_name}")
                logger.info(f"ğŸ”§ Compute: {compute_cap}, Memory: {memory_gb:.1f}GB")
                
                # Test basic operations
                test_tensor = torch.ones(10, device='cuda', dtype=torch.float32)
                test_result = test_tensor.sum()
                del test_tensor, test_result
                torch.cuda.empty_cache()
                
                # Test embedding operations (critical for text encoder)
                test_embed = torch.nn.Embedding(50, 16, dtype=torch.float32).cuda()
                test_input = torch.randint(0, 50, (2, 5), device='cuda')
                test_output = test_embed(test_input)
                del test_embed, test_input, test_output
                torch.cuda.empty_cache()
                
                logger.info("âœ… CUDA compatibility verification passed")
                return True
                
            except RuntimeError as e:
                error_str = str(e).lower()
                if any(keyword in error_str for keyword in ['kernel', 'no kernel', 'compatibility']):
                    logger.error(f"âŒ CUDA kernel compatibility issue: {e}")
                    # Apply additional compatibility measures
                    torch.backends.cudnn.enabled = False
                    logger.info("ğŸ”„ Applied CUDNN disable fix")
                    return True  # Continue with CUDNN disabled
                else:
                    raise e
        else:
            logger.warning("âš ï¸ CUDA not available, running in CPU mode")
            return False
            
    except Exception as e:
        logger.error(f"âŒ PyTorch setup failed: {e}")
        return False

# Initialize CUDA safety
CUDA_AVAILABLE = setup_pytorch_production()

# ================== SAFE COMFYUI IMPORTS ==================
def safe_import_comfyui():
    """ğŸ”§ Production-safe ComfyUI import vá»›i comprehensive error handling"""
    try:
        logger.info("ğŸ“¦ Importing ComfyUI modules...")
        
        # Core ComfyUI nodes
        from nodes import (
            CheckpointLoaderSimple, CLIPLoader, CLIPTextEncode, VAEDecode, VAELoader,
            KSampler, KSamplerAdvanced, UNETLoader, LoadImage, SaveImage,
            CLIPVisionLoader, CLIPVisionEncode, LoraLoaderModelOnly, ImageScale
        )
        
        # GGUF support
        from custom_nodes.ComfyUI_GGUF.nodes import UnetLoaderGGUF
        
        # Optimization nodes
        from custom_nodes.ComfyUI_KJNodes.nodes.model_optimization_nodes import (
            WanVideoTeaCacheKJ, PathchSageAttentionKJ, WanVideoNAG, SkipLayerGuidanceWanVideo
        )
        
        # Advanced nodes
        from comfy_extras.nodes_model_advanced import ModelSamplingSD3
        from comfy_extras.nodes_images import SaveAnimatedWEBP
        from comfy_extras.nodes_video import SaveWEBM
        from comfy_extras.nodes_wan import WanImageToVideo
        from comfy import model_management
        import folder_paths
        
        logger.info("âœ… ComfyUI modules imported successfully")
        
        return True, {
            'CLIPLoader': CLIPLoader,
            'CLIPTextEncode': CLIPTextEncode,
            'VAEDecode': VAEDecode,
            'VAELoader': VAELoader,
            'UnetLoaderGGUF': UnetLoaderGGUF,
            'LoadImage': LoadImage,
            'ImageScale': ImageScale,
            'WanImageToVideo': WanImageToVideo,
            'KSamplerAdvanced': KSamplerAdvanced,
            'LoraLoaderModelOnly': LoraLoaderModelOnly,
            'PathchSageAttentionKJ': PathchSageAttentionKJ,
            'WanVideoTeaCacheKJ': WanVideoTeaCacheKJ,
            'WanVideoNAG': WanVideoNAG,
            'ModelSamplingSD3': ModelSamplingSD3,
            'CLIPVisionLoader': CLIPVisionLoader,
            'CLIPVisionEncode': CLIPVisionEncode
        }
        
    except ImportError as e:
        logger.error(f"âŒ ComfyUI import failed: {e}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        return False, {}

# Import ComfyUI components
COMFYUI_AVAILABLE, COMFYUI_NODES = safe_import_comfyui()

# ================== MODEL & STORAGE CONFIGURATIONS ==================
MODEL_CONFIGS = {
    # Q6_K DIT Models
    "dit_model_high": "/app/ComfyUI/models/diffusion_models/wan2.2_i2v_high_noise_14B_Q6_K.gguf",
    "dit_model_low": "/app/ComfyUI/models/diffusion_models/wan2.2_i2v_low_noise_14B_Q6_K.gguf",
    
    # Supporting models
    "text_encoder": "/app/ComfyUI/models/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors",
    "vae": "/app/ComfyUI/models/vae/wan_2.1_vae.safetensors",
    "clip_vision": "/app/ComfyUI/models/clip_vision/clip_vision_h.safetensors",
    
    # LightX2V LoRAs
    "lightx2v_rank_32": "/app/ComfyUI/models/loras/lightx2v_I2V_14B_480p_cfg_step_distill_rank32_bf16.safetensors",
    "lightx2v_rank_64": "/app/ComfyUI/models/loras/lightx2v_T2V_14B_cfg_step_distill_v2_lora_rank64_bf16.safetensors",
    "lightx2v_rank_128": "/app/ComfyUI/models/loras/lightx2v_T2V_14B_cfg_step_distill_v2_lora_rank128_bf16.safetensors",
    
    # Built-in LoRAs
    "walking_to_viewers": "/app/ComfyUI/models/loras/walking to viewers_Wan.safetensors",
    "walking_from_behind": "/app/ComfyUI/models/loras/walking_from_behind.safetensors",
    "dancing": "/app/ComfyUI/models/loras/b3ll13-d8nc3r.safetensors",
    "pusa_lora": "/app/ComfyUI/models/loras/Wan21_PusaV1_LoRA_14B_rank512_bf16.safetensors",
    "rotate_lora": "/app/ComfyUI/models/loras/rotate_20_epochs.safetensors"
}

# MinIO Storage Configuration
MINIO_ENDPOINT = "media.aiclip.ai"
MINIO_ACCESS_KEY = "VtZ6MUPfyTOH3qSiohA2"
MINIO_SECRET_KEY = "8boVPVIynLEKcgXirrcePxvjSk7gReIDD9pwto3t"
MINIO_BUCKET = "video"
MINIO_SECURE = False

try:
    minio_client = Minio(
        MINIO_ENDPOINT,
        access_key=MINIO_ACCESS_KEY,
        secret_key=MINIO_SECRET_KEY,
        secure=MINIO_SECURE
    )
    logger.info("âœ… MinIO client initialized")
except Exception as e:
    logger.error(f"âŒ MinIO initialization failed: {e}")
    minio_client = None

# ================== UTILITY FUNCTIONS ==================

def clear_memory_comprehensive():
    """ğŸ§¹ Comprehensive memory cleanup vá»›i enhanced CUDA management"""
    try:
        # Python garbage collection
        collected = gc.collect()
        
        if CUDA_AVAILABLE and torch.cuda.is_available():
            # Multi-stage CUDA cleanup
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()
            torch.cuda.synchronize()
            
            # Reset memory statistics
            torch.cuda.reset_peak_memory_stats()
            torch.cuda.reset_accumulated_memory_stats()
            
            # Force memory cleanup
            if hasattr(torch.cuda, 'memory_summary'):
                try:
                    torch.cuda.memory_summary(device=None, abbreviated=True)
                except:
                    pass
                    
            logger.debug(f"ğŸ§¹ Memory cleanup: collected {collected} objects")
        
    except Exception as e:
        logger.warning(f"âš ï¸ Memory cleanup warning: {e}")

def verify_models_comprehensive() -> tuple[bool, list]:
    """ğŸ” Comprehensive model verification vá»›i detailed reporting"""
    logger.info("ğŸ” Verifying all required models...")
    missing_models = []
    existing_models = []
    total_size = 0
    
    for name, path in MODEL_CONFIGS.items():
        if os.path.exists(path):
            try:
                file_size_mb = os.path.getsize(path) / (1024 * 1024)
                existing_models.append(f"{name}: {file_size_mb:.1f}MB")
                total_size += file_size_mb
                logger.info(f"âœ… {name}: {file_size_mb:.1f}MB")
                
                # Verify file integrity (basic check)
                if file_size_mb < 1:  # Suspiciously small
                    logger.warning(f"âš ï¸ {name} seems too small: {file_size_mb:.1f}MB")
                    
            except Exception as e:
                logger.error(f"âŒ Error checking {name}: {e}")
                missing_models.append(f"{name}: {path} (read error)")
        else:
            missing_models.append(f"{name}: {path}")
            logger.error(f"âŒ Missing: {name}")
    
    if missing_models:
        logger.error(f"âŒ Missing {len(missing_models)}/{len(MODEL_CONFIGS)} models")
        return False, missing_models
    else:
        logger.info(f"âœ… All {len(existing_models)} models verified! Total: {total_size:.1f}MB")
        return True, []

def get_lightx2v_lora_path(lightx2v_rank: str) -> str:
    """ğŸ“ Get LightX2V LoRA path by rank vá»›i validation"""
    rank_mapping = {
        "32": "lightx2v_rank_32",
        "64": "lightx2v_rank_64",
        "128": "lightx2v_rank_128"
    }
    config_key = rank_mapping.get(lightx2v_rank, "lightx2v_rank_32")
    path = MODEL_CONFIGS.get(config_key)
    
    if path and os.path.exists(path):
        return path
    else:
        logger.warning(f"âš ï¸ LightX2V rank {lightx2v_rank} not found, using default")
        return MODEL_CONFIGS.get("lightx2v_rank_32")

def download_lora_dynamic(lora_url: str, civitai_token: str = None) -> str:
    """ğŸ¨ Enhanced LoRA download vá»›i comprehensive error handling"""
    try:
        lora_dir = "/app/ComfyUI/models/loras"
        os.makedirs(lora_dir, exist_ok=True)
        
        logger.info(f"ğŸ¨ Downloading LoRA: {lora_url}")
        
        if "huggingface.co" in lora_url:
            # HuggingFace download
            parts = lora_url.split("/")
            if len(parts) >= 7 and "resolve" in parts:
                username = parts[3]
                repo = parts[4]
                filename = parts[-1]
                
                logger.info(f"ğŸ“¥ HuggingFace: {username}/{repo}/{filename}")
                downloaded_path = hf_hub_download(
                    repo_id=f"{username}/{repo}",
                    filename=filename,
                    local_dir=lora_dir,
                    force_download=True
                )
                
                # Verify download
                if os.path.exists(downloaded_path) and os.path.getsize(downloaded_path) > 0:
                    size_mb = os.path.getsize(downloaded_path) / (1024 * 1024)
                    logger.info(f"âœ… HuggingFace LoRA downloaded: {filename} ({size_mb:.1f}MB)")
                    return downloaded_path
                else:
                    logger.error("âŒ HuggingFace download verification failed")
                    return None
                
        elif "civitai.com" in lora_url:
            # CivitAI download vá»›i enhanced handling
            try:
                if "/models/" in lora_url:
                    model_id = lora_url.split("/models/")[1].split("?")[0].split("/")[0]
                else:
                    raise ValueError("Invalid CivitAI URL format")
                
                headers = {}
                if civitai_token:
                    headers["Authorization"] = f"Bearer {civitai_token}"
                
                api_url = f"https://civitai.com/api/download/models/{model_id}?type=Model&format=SafeTensor"
                logger.info(f"ğŸ“¥ CivitAI: model_id={model_id}")
                
                response = requests.get(api_url, headers=headers, timeout=300, stream=True)
                response.raise_for_status()
                
                timestamp = int(time.time())
                filename = f"civitai_{model_id}_{timestamp}.safetensors"
                local_path = os.path.join(lora_dir, filename)
                
                total_size = int(response.headers.get('content-length', 0))
                downloaded = 0
                
                with open(local_path, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
                            downloaded += len(chunk)
                            
                            # Progress logging every 10MB
                            if total_size > 0 and downloaded % (1024 * 1024 * 10) == 0:
                                progress = (downloaded / total_size) * 100
                                logger.info(f"ğŸ“¥ Download progress: {progress:.1f}%")
                
                # Verify download
                if os.path.exists(local_path) and os.path.getsize(local_path) > 0:
                    size_mb = downloaded / (1024 * 1024)
                    logger.info(f"âœ… CivitAI LoRA downloaded: {filename} ({size_mb:.1f}MB)")
                    return local_path
                else:
                    logger.error("âŒ CivitAI download verification failed")
                    return None
                
            except Exception as e:
                logger.error(f"âŒ CivitAI download failed: {e}")
                return None
        else:
            # Direct download
            filename = os.path.basename(urlparse(lora_url).path)
            if not filename.endswith(('.safetensors', '.ckpt', '.pt', '.pth')):
                filename = f"direct_lora_{int(time.time())}.safetensors"
            
            local_path = os.path.join(lora_dir, filename)
            
            response = requests.get(lora_url, timeout=300, stream=True)
            response.raise_for_status()
            
            with open(local_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
            
            # Verify download
            if os.path.exists(local_path) and os.path.getsize(local_path) > 0:
                size_mb = os.path.getsize(local_path) / (1024 * 1024)
                logger.info(f"âœ… Direct LoRA downloaded: {filename} ({size_mb:.1f}MB)")
                return local_path
            else:
                logger.error("âŒ Direct download verification failed")
                return None
        
        return None
        
    except Exception as e:
        logger.error(f"âŒ LoRA download failed: {e}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        return None

# ================== ENHANCED ASPECT RATIO PROCESSING ==================

def calculate_aspect_preserving_dimensions(original_width, original_height, target_width, target_height):
    """ğŸ“ Calculate dimensions preserving aspect ratio vá»›i enhanced logic"""
    original_ratio = original_width / original_height
    target_ratio = target_width / target_height
    
    if abs(original_ratio - target_ratio) < 0.01:  # Already close to target ratio
        return target_width, target_height
    
    if original_ratio > target_ratio:
        # Wider image -> pad top/bottom
        new_width = target_width
        new_height = int(target_width / original_ratio)
    else:
        # Taller image -> pad left/right
        new_height = target_height
        new_width = int(target_height * original_ratio)
    
    # Ensure divisible by 8 for video encoding
    new_width = max(8, (new_width // 8) * 8)
    new_height = max(8, (new_height // 8) * 8)
    
    return new_width, new_height

def calculate_crop_dimensions(original_width, original_height, target_width, target_height):
    """âœ‚ï¸ Calculate smart crop dimensions vá»›i center focus"""
    target_ratio = target_width / target_height
    original_ratio = original_width / original_height
    
    if original_ratio > target_ratio:
        # Wider image -> crop width (center crop)
        crop_height = original_height
        crop_width = int(original_height * target_ratio)
        crop_x = max(0, (original_width - crop_width) // 2)
        crop_y = 0
    else:
        # Taller image -> crop height (center crop)
        crop_width = original_width
        crop_height = int(original_width / target_ratio)
        crop_x = 0
        crop_y = max(0, (original_height - crop_height) // 2)
    
    # Ensure crop dimensions are valid
    crop_width = min(crop_width, original_width)
    crop_height = min(crop_height, original_height)
    crop_x = min(crop_x, original_width - crop_width)
    crop_y = min(crop_y, original_height - crop_height)
    
    return crop_x, crop_y, crop_width, crop_height

def process_image_with_enhanced_aspect_control(
    image_path: str,
    target_width: int = 720,
    target_height: int = 1280,
    aspect_mode: str = "preserve",
    auto_720p: bool = False
):
    """
    ğŸ–¼ï¸ PRODUCTION: Enhanced image processing vá»›i comprehensive tensor validation
    Supports all popular image sizes vÃ  aspect ratios
    """
    try:
        if not COMFYUI_AVAILABLE:
            raise RuntimeError("ComfyUI not available")
        
        load_image = COMFYUI_NODES['LoadImage']()
        image_scaler = COMFYUI_NODES['ImageScale']()
        
        # Load vÃ  validate original image
        loaded_image = load_image.load_image(image_path)[0]
        
        logger.info(f"ğŸ“Š Initial image tensor: shape={loaded_image.shape}, dtype={loaded_image.dtype}")
        
        # Normalize tensor dimensions
        if loaded_image.ndim == 4:
            batch, orig_height, orig_width, channels = loaded_image.shape
            if batch != 1:
                logger.warning(f"âš ï¸ Unexpected batch size: {batch}")
                loaded_image = loaded_image[0:1]
        elif loaded_image.ndim == 3:
            orig_height, orig_width, channels = loaded_image.shape
            loaded_image = loaded_image.unsqueeze(0)  # Add batch dimension
        else:
            raise ValueError(f"Invalid image tensor shape: {loaded_image.shape}")
        
        logger.info(f"ğŸ–¼ï¸ Original image: {orig_width}x{orig_height} (channels: {channels})")
        
        # Auto 720p detection vá»›i enhanced logic
        if auto_720p:
            aspect_ratio = orig_width / orig_height
            
            if aspect_ratio > 1.3:  # Landscape
                target_width, target_height = 1280, 720
            elif aspect_ratio < 0.8:  # Portrait
                target_width, target_height = 720, 1280
            else:  # Square-ish, choose based on original size
                if orig_width > orig_height:
                    target_width, target_height = 1280, 720
                else:
                    target_width, target_height = 720, 1280
                    
            logger.info(f"ğŸ“± Auto 720p: {orig_width}x{orig_height} -> {target_width}x{target_height}")
        
        # Process based on aspect mode vá»›i comprehensive validation
        if aspect_mode == "preserve":
            # Calculate new dimensions preserving aspect ratio
            new_width, new_height = calculate_aspect_preserving_dimensions(
                orig_width, orig_height, target_width, target_height
            )
            
            logger.info(f"ğŸ“ Preserve dimensions: {new_width}x{new_height}")
            
            # Scale to calculated dimensions
            scaled_image = image_scaler.upscale(
                loaded_image, "lanczos", new_width, new_height, "disabled"
            )[0]
            
            logger.info(f"ğŸ“Š Scaled tensor: {scaled_image.shape}")
            
            # Add padding if needed
            if new_width != target_width or new_height != target_height:
                logger.info(f"ğŸ”§ Adding padding: {new_width}x{new_height} -> {target_width}x{target_height}")
                
                # Calculate symmetric padding
                pad_x = (target_width - new_width) // 2
                pad_y = (target_height - new_height) // 2
                pad_x_right = target_width - new_width - pad_x
                pad_y_bottom = target_height - new_height - pad_y
                
                logger.info(f"ğŸ“ Padding: left={pad_x}, right={pad_x_right}, top={pad_y}, bottom={pad_y_bottom}")
                
                # Ensure proper tensor format
                if scaled_image.ndim == 3:
                    scaled_image = scaled_image.unsqueeze(0)
                
                # Convert BHWC -> BCHW for padding
                scaled_bchw = scaled_image.permute(0, 3, 1, 2)
                
                # Apply padding vá»›i validation
                try:
                    padded_bchw = F.pad(
                        scaled_bchw,
                        (pad_x, pad_x_right, pad_y, pad_y_bottom),
                        mode='constant',
                        value=0
                    )
                    
                    # Convert back BCHW -> HWC
                    final_image = padded_bchw.permute(0, 2, 3, 1)[0]
                    
                except Exception as pad_error:
                    logger.error(f"âŒ Padding failed: {pad_error}")
                    logger.info("ğŸ”„ Fallback to direct scaling")
                    final_image = image_scaler.upscale(
                        loaded_image, "lanczos", target_width, target_height, "disabled"
                    )[0]
            else:
                final_image = scaled_image if scaled_image.ndim == 3 else scaled_image[0]
                
        elif aspect_mode == "crop":
            # Smart crop preserving aspect ratio
            crop_x, crop_y, crop_width, crop_height = calculate_crop_dimensions(
                orig_width, orig_height, target_width, target_height
            )
            
            logger.info(f"âœ‚ï¸ Crop: ({crop_x},{crop_y}) {crop_width}x{crop_height}")
            
            # Perform crop vá»›i bounds checking
            if loaded_image.ndim == 4:
                cropped = loaded_image[0, crop_y:crop_y+crop_height, crop_x:crop_x+crop_width, :]
                cropped = cropped.unsqueeze(0)
            else:
                cropped = loaded_image[crop_y:crop_y+crop_height, crop_x:crop_x+crop_width, :]
                cropped = cropped.unsqueeze(0)
            
            # Scale to target size
            final_image = image_scaler.upscale(
                cropped, "lanczos", target_width, target_height, "disabled"
            )[0]
            
        else:  # stretch
            # Direct stretch (backward compatible)
            final_image = image_scaler.upscale(
                loaded_image, "lanczos", target_width, target_height, "disabled"
            )[0]
        
        # COMPREHENSIVE FINAL VALIDATION
        logger.info(f"ğŸ” Final image validation:")
        logger.info(f"  Shape: {final_image.shape}")
        logger.info(f"  Expected: [{target_height}, {target_width}, 3]")
        
        # Validate tensor properties
        if final_image.ndim != 3:
            raise ValueError(f"Final image must be 3D [H,W,C], got {final_image.ndim}D")
        
        actual_h, actual_w, actual_c = final_image.shape
        
        # Dimension validation
        if actual_h != target_height or actual_w != target_width:
            logger.error(f"âŒ Size mismatch: {actual_h}x{actual_w} vs {target_height}x{target_width}")
            raise ValueError(f"Size mismatch: expected {target_height}x{target_width}, got {actual_h}x{actual_w}")
        
        # Channel validation vÃ  normalization
        if actual_c == 1:
            logger.info("ğŸ”„ Converting grayscale to RGB...")
            final_image = final_image.repeat(1, 1, 3)
        elif actual_c == 4:
            logger.info("ğŸ”„ Removing alpha channel...")
            final_image = final_image[:, :, :3]
        elif actual_c != 3:
            raise ValueError(f"Invalid channel count: {actual_c}")
        
        # Final validation
        final_h, final_w, final_c = final_image.shape
        if final_c != 3:
            raise ValueError(f"Final image must have 3 channels, got {final_c}")
        
        logger.info(f"âœ… Image processing completed: {final_w}x{final_h} RGB")
        logger.info(f"ğŸ“Š Mode: {aspect_mode}, Auto720p: {auto_720p}")
        
        return final_image, target_width, target_height
        
    except Exception as e:
        logger.error(f"âŒ Image processing failed: {e}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        raise e

# ================== CUDA-SAFE TEXT PROCESSING ==================

def safe_load_text_encoder():
    """ğŸ”§ Production-safe text encoder loading vá»›i comprehensive fallbacks"""
    try:
        clear_memory_comprehensive()
        
        if not COMFYUI_AVAILABLE:
            raise RuntimeError("ComfyUI not available")
        
        clip_loader = COMFYUI_NODES['CLIPLoader']()
        
        # Strategy 1: Normal loading
        try:
            logger.info("ğŸ“ Loading text encoder...")
            clip = clip_loader.load_clip("umt5_xxl_fp8_e4m3fn_scaled.safetensors", "wan", "default")[0]
            
            # Test functionality
            clip_encode = COMFYUI_NODES['CLIPTextEncode']()
            test_result = clip_encode.encode(clip, "test")
            del test_result
            
            logger.info("âœ… Text encoder loaded successfully")
            return clip
            
        except RuntimeError as e:
            error_str = str(e).lower()
            if any(keyword in error_str for keyword in ['cuda error', 'kernel', 'no kernel']):
                logger.warning("âš ï¸ CUDA compatibility issue, applying fixes...")
                
                # Strategy 2: Disable CUDNN
                original_cudnn = torch.backends.cudnn.enabled
                torch.backends.cudnn.enabled = False
                
                try:
                    clear_memory_comprehensive()
                    clip = clip_loader.load_clip("umt5_xxl_fp8_e4m3fn_scaled.safetensors", "wan", "default")[0]
                    logger.info("âœ… Text encoder loaded with CUDNN disabled")
                    return clip
                except Exception:
                    torch.backends.cudnn.enabled = original_cudnn
                
                # Strategy 3: Different device context
                try:
                    clear_memory_comprehensive()
                    with torch.cuda.device(0):
                        clip = clip_loader.load_clip("umt5_xxl_fp8_e4m3fn_scaled.safetensors", "wan", "default")[0]
                    logger.info("âœ… Text encoder loaded with device context")
                    return clip
                except Exception:
                    pass
                
                # Strategy 4: CPU fallback (last resort)
                logger.warning("ğŸ”„ CPU fallback for text encoder...")
                try:
                    original_cuda_available = torch.cuda.is_available
                    torch.cuda.is_available = lambda: False
                    
                    clip = clip_loader.load_clip("umt5_xxl_fp8_e4m3fn_scaled.safetensors", "wan", "default")[0]
                    
                    torch.cuda.is_available = original_cuda_available
                    logger.info("âœ… Text encoder loaded in CPU mode")
                    return clip
                except Exception:
                    torch.cuda.is_available = original_cuda_available
                    raise e
            else:
                raise e
                
    except Exception as e:
        logger.error(f"âŒ Text encoder loading failed: {e}")
        raise e

def safe_encode_text(clip, text):
    """ğŸ”§ CUDA-safe text encoding vá»›i comprehensive retry logic"""
    try:
        clip_encode = COMFYUI_NODES['CLIPTextEncode']()
        
        max_retries = 3
        for attempt in range(max_retries):
            try:
                result = clip_encode.encode(clip, text)
                return result[0]
                
            except RuntimeError as e:
                error_str = str(e).lower()
                if any(keyword in error_str for keyword in ['cuda error', 'kernel']) and attempt < max_retries - 1:
                    logger.warning(f"âš ï¸ CUDA error on attempt {attempt + 1}, retrying...")
                    clear_memory_comprehensive()
                    time.sleep(1)  # Brief pause
                    continue
                else:
                    raise e
                    
    except Exception as e:
        logger.error(f"âŒ Text encoding failed: {e}")
        raise e

# ================== RIFE INTERPOLATION (PRODUCTION FIXED) ==================

def apply_rife_interpolation_production_final(video_path: str, interpolation_factor: int = 2) -> str:
    """ğŸ”§ PRODUCTION FINAL: RIFE interpolation vá»›i ultimate output detection"""
    try:
        logger.info(f"ğŸ”„ RIFE interpolation (factor: {interpolation_factor}) - PRODUCTION FINAL...")
        
        # Validate input
        if not os.path.exists(video_path):
            logger.error(f"âŒ Video not found: {video_path}")
            return video_path
        
        original_size = os.path.getsize(video_path) / (1024 * 1024)
        logger.info(f"ğŸ“Š Original: {original_size:.1f}MB")
        
        # Check RIFE availability
        rife_script = "/app/Practical-RIFE/inference_video.py"
        if not os.path.exists(rife_script):
            logger.warning(f"âš ï¸ RIFE not available: {rife_script}")
            return video_path
        
        # Setup directories
        rife_output_dir = "/app/rife_output"
        os.makedirs(rife_output_dir, exist_ok=True)
        
        # Generate output path
        input_basename = os.path.splitext(os.path.basename(video_path))[0]
        final_output_path = os.path.join(rife_output_dir, f"{input_basename}_rife_{interpolation_factor}x.mp4")
        
        # Copy input to RIFE directory
        rife_input_path = os.path.join("/app/Practical-RIFE", os.path.basename(video_path))
        shutil.copy2(video_path, rife_input_path)
        
        # Enhanced environment
        env = os.environ.copy()
        env.update({
            "CUDA_VISIBLE_DEVICES": "0",
            "PYTHONPATH": "/app/Practical-RIFE",
            "SDL_AUDIODRIVER": "dummy",
            "FFMPEG_LOGLEVEL": "quiet"
        })
        
        # Build RIFE command
        cmd = [
            "python3", "inference_video.py",
            f"--multi={interpolation_factor}",
            f"--video={os.path.basename(video_path)}",
            "--scale=1.0",
            "--fps=30"
        ]
        
        logger.info(f"ğŸ”§ RIFE command: {' '.join(cmd)}")
        
        # Execute RIFE
        original_cwd = os.getcwd()
        os.chdir("/app/Practical-RIFE")
        
        try:
            # Clean previous outputs
            for pattern in ["*_interpolated.mp4", "*X.mp4", "*_rife.mp4"]:
                for file in Path(".").glob(pattern):
                    try:
                        file.unlink()
                    except:
                        pass
            
            # Run RIFE vá»›i timeout
            start_time = time.time()
            result = subprocess.run(
                cmd, env=env, capture_output=True, text=True,
                timeout=600, check=False  # 10 minutes timeout
            )
            
            execution_time = time.time() - start_time
            logger.info(f"â±ï¸ RIFE execution: {execution_time:.1f}s, code: {result.returncode}")
            
        except subprocess.TimeoutExpired:
            logger.error("âŒ RIFE timed out")
            return video_path
        except Exception as e:
            logger.error(f"âŒ RIFE execution failed: {e}")
            return video_path
        finally:
            os.chdir(original_cwd)
            # Cleanup input copy
            try:
                if os.path.exists(rife_input_path):
                    os.remove(rife_input_path)
            except:
                pass
        
        # ULTIMATE OUTPUT DETECTION
        logger.info("ğŸ” Ultimate output detection...")
        
        potential_outputs = []
        rife_dir = "/app/Practical-RIFE"
        input_name = os.path.splitext(os.path.basename(video_path))[0]
        
        # Comprehensive search patterns
        patterns = [
            f"{input_name}_{interpolation_factor}X.mp4",
            f"{input_name}_{interpolation_factor}x.mp4",
            f"{input_name}_interpolated.mp4",
            f"{input_name}_rife.mp4",
            f"output_{interpolation_factor}x.mp4",
            "output.mp4", "interpolated.mp4",
            os.path.basename(video_path).replace('.mp4', f'_{interpolation_factor}X.mp4'),
            os.path.basename(video_path).replace('.mp4', '_interpolated.mp4')
        ]
        
        # Search by patterns
        for pattern in patterns:
            path = os.path.join(rife_dir, pattern)
            if os.path.exists(path) and path not in potential_outputs:
                potential_outputs.append(path)
                logger.info(f"ğŸ” Found: {pattern}")
        
        # Search by recent modification
        current_time = time.time()
        try:
            for file in os.listdir(rife_dir):
                if file.endswith('.mp4') and file != os.path.basename(video_path):
                    file_path = os.path.join(rife_dir, file)
                    # Check if created in last 15 minutes
                    if current_time - max(os.path.getctime(file_path), os.path.getmtime(file_path)) < 900:
                        if file_path not in potential_outputs:
                            potential_outputs.append(file_path)
                            logger.info(f"ğŸ” Recent: {file}")
        except Exception as e:
            logger.warning(f"âš ï¸ Directory scan failed: {e}")
        
        # Select best output
        selected_output = None
        if potential_outputs:
            logger.info(f"ğŸ“Š Analyzing {len(potential_outputs)} candidates:")
            
            best_candidate = None
            best_score = 0
            
            for path in potential_outputs:
                try:
                    size_mb = os.path.getsize(path) / (1024 * 1024)
                    mod_time = os.path.getmtime(path)
                    
                    # Scoring: size relevance + recency
                    size_score = min(size_mb / original_size, 5.0)
                    recency_score = max(0, (900 - (current_time - mod_time)) / 900)
                    total_score = size_score * 0.8 + recency_score * 0.2
                    
                    logger.info(f"  ğŸ“„ {os.path.basename(path)}: {size_mb:.1f}MB, score: {total_score:.2f}")
                    
                    if (size_mb >= original_size * 0.3 and  # Minimum 30% of original
                        size_mb <= original_size * 20 and    # Maximum 20x original
                        total_score > best_score):
                        
                        best_candidate = path
                        best_score = total_score
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ Error analyzing {path}: {e}")
            
            selected_output = best_candidate
        
        # Process selected output
        if selected_output and os.path.exists(selected_output):
            try:
                shutil.move(selected_output, final_output_path)
                
                final_size = os.path.getsize(final_output_path) / (1024 * 1024)
                logger.info(f"âœ… RIFE interpolation successful!")
                logger.info(f"ğŸ“Š {original_size:.1f}MB â†’ {final_size:.1f}MB ({final_size/original_size:.1f}x)")
                logger.info(f"â±ï¸ Processing time: {execution_time:.1f}s")
                
                return final_output_path
                
            except Exception as e:
                logger.error(f"âŒ Failed to move output: {e}")
                return video_path
        else:
            # Debug logging
            logger.warning("ğŸ” COMPREHENSIVE DEBUG - No valid output found")
            try:
                logger.warning("ğŸ“ RIFE directory contents:")
                for item in os.listdir(rife_dir):
                    if os.path.isfile(os.path.join(rife_dir, item)):
                        size = os.path.getsize(os.path.join(rife_dir, item)) / (1024 * 1024)
                        logger.warning(f"  ğŸ“„ {item}: {size:.1f}MB")
            except Exception as e:
                logger.error(f"âŒ Debug listing failed: {e}")
            
            logger.warning("âš ï¸ Using original video")
            return video_path
            
    except Exception as e:
        logger.error(f"âŒ RIFE interpolation error: {e}")
        return video_path

# ================== PRODUCTION VIDEO SAVING ==================

def save_video_production_final(frames_tensor, output_path, fps=16):
    """ğŸ¬ PRODUCTION FINAL: Multi-strategy video saving vá»›i comprehensive validation"""
    try:
        logger.info(f"ğŸ¬ Saving video: {output_path}")
        
        if frames_tensor is None:
            raise ValueError("Frames tensor is None")
        
        # Convert to numpy vá»›i validation
        if torch.is_tensor(frames_tensor):
            frames_np = frames_tensor.detach().cpu().float().numpy()
        else:
            frames_np = np.array(frames_tensor, dtype=np.float32)
        
        logger.info(f"ğŸ“Š Input: shape={frames_np.shape}, dtype={frames_np.dtype}, range=[{frames_np.min():.3f}, {frames_np.max():.3f}]")
        
        # Handle batch dimension
        if frames_np.ndim == 5 and frames_np.shape[0] == 1:
            frames_np = frames_np[0]
            logger.info(f"ğŸ“Š Batch removed: {frames_np.shape}")
        
        # Convert to uint8
        if frames_np.dtype != np.uint8:
            if frames_np.max() <= 1.0:
                frames_np = (frames_np * 255.0).astype(np.uint8)
            else:
                frames_np = np.clip(frames_np, 0, 255).astype(np.uint8)
        
        # Validate dimensions
        if len(frames_np.shape) != 4:
            raise ValueError(f"Expected 4D tensor [frames, height, width, channels], got {frames_np.shape}")
        
        num_frames, height, width, channels = frames_np.shape
        logger.info(f"ğŸ“Š Video specs: {num_frames} frames, {width}x{height}, {channels} channels")
        
        if num_frames == 0:
            raise ValueError("No frames to save")
        
        # Handle different channel configurations
        if channels == 1:
            logger.info("ğŸ”„ Converting grayscale to RGB...")
            frames_np = np.repeat(frames_np, 3, axis=-1)
            channels = 3
        elif channels == 4:
            logger.info("ğŸ”„ Removing alpha channel...")
            frames_np = frames_np[:, :, :, :3]
            channels = 3
        elif channels != 3:
            raise ValueError(f"Unsupported channel count: {channels}")
        
        # Create output directory
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # Multi-strategy encoding
        strategies = [
            {
                'name': 'High Quality',
                'params': {
                    'fps': fps,
                    'codec': 'libx264',
                    'pixelformat': 'yuv420p',
                    'quality': 8,
                    'bitrate': '5M'
                }
            },
            {
                'name': 'Standard',
                'params': {
                    'fps': fps,
                    'codec': 'libx264',
                    'pixelformat': 'yuv420p'
                }
            },
            {
                'name': 'Basic',
                'params': {'fps': fps}
            },
            {
                'name': 'Fallback',
                'params': {
                    'fps': fps,
                    'codec': 'libx264',
                    'macro_block_size': None
                }
            }
        ]
        
        # Try encoding strategies
        for strategy in strategies:
            try:
                logger.info(f"ğŸ¥ Trying {strategy['name']} encoding...")
                
                with imageio.get_writer(output_path, **strategy['params']) as writer:
                    for i, frame in enumerate(frames_np):
                        writer.append_data(frame)
                        
                        # Progress logging
                        if (i + 1) % max(1, num_frames // 5) == 0:
                            progress = ((i + 1) / num_frames) * 100
                            logger.info(f"ğŸ“¹ Progress: {progress:.1f}% ({i + 1}/{num_frames})")
                
                # Verify output
                if os.path.exists(output_path) and os.path.getsize(output_path) > 0:
                    file_size_mb = os.path.getsize(output_path) / (1024 * 1024)
                    
                    # Basic sanity check
                    if file_size_mb < 0.01:  # Less than 10KB is suspicious
                        logger.warning(f"âš ï¸ {strategy['name']} produced suspiciously small file: {file_size_mb:.3f}MB")
                        continue
                    
                    logger.info(f"âœ… Video saved with {strategy['name']}!")
                    logger.info(f"ğŸ“Š Final: {file_size_mb:.2f}MB, {num_frames} frames @ {fps}fps")
                    
                    return output_path
                else:
                    logger.warning(f"âš ï¸ {strategy['name']} produced empty file")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ {strategy['name']} failed: {e}")
                continue
        
        raise RuntimeError("All encoding strategies failed")
        
    except Exception as e:
        logger.error(f"âŒ Video saving failed: {e}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        raise e

# ================== MAIN VIDEO GENERATION (PRODUCTION FINAL) ==================

def generate_video_wan22_production_final(image_path: str, **kwargs) -> str:
    """
    ğŸ¬ PRODUCTION FINAL: WAN2.2 video generation vá»›i comprehensive optimization
    Supports all popular image sizes vÃ  aspect ratios
    """
    try:
        logger.info("ğŸ¬ Starting WAN2.2 PRODUCTION FINAL generation...")
        
        # Parameter extraction vá»›i comprehensive defaults
        positive_prompt = kwargs.get('positive_prompt', '')
        negative_prompt = kwargs.get('negative_prompt', 'è‰²è°ƒè‰³ä¸½ï¼Œè¿‡æ›ï¼Œé™æ€ï¼Œç»†èŠ‚æ¨¡ç³Šä¸æ¸…ï¼Œå­—å¹•ï¼Œé£æ ¼ï¼Œä½œå“ï¼Œç”»ä½œï¼Œç”»é¢ï¼Œé™æ­¢ï¼Œæ•´ä½“å‘ç°ï¼Œæœ€å·®è´¨é‡ï¼Œä½è´¨é‡ï¼ŒJPEGå‹ç¼©æ®‹ç•™ï¼Œä¸‘é™‹çš„ï¼Œæ®‹ç¼ºçš„ï¼Œå¤šä½™çš„æ‰‹æŒ‡ï¼Œç”»å¾—ä¸å¥½çš„æ‰‹éƒ¨ï¼Œç”»å¾—ä¸å¥½çš„è„¸éƒ¨ï¼Œç•¸å½¢çš„ï¼Œæ¯å®¹çš„ï¼Œå½¢æ€ç•¸å½¢çš„è‚¢ä½“ï¼Œæ‰‹æŒ‡èåˆï¼Œé™æ­¢ä¸åŠ¨çš„ç”»é¢ï¼Œæ‚ä¹±çš„èƒŒæ™¯ï¼Œä¸‰æ¡è…¿ï¼ŒèƒŒæ™¯äººå¾ˆå¤šï¼Œå€’ç€èµ°')
        
        # Enhanced aspect control
        aspect_mode = kwargs.get('aspect_mode', 'preserve')
        auto_720p = kwargs.get('auto_720p', False)
        
        # Video settings
        width = kwargs.get('width', 720)
        height = kwargs.get('height', 1280)
        seed = kwargs.get('seed', 0)
        steps = kwargs.get('steps', 6)
        high_noise_steps = kwargs.get('high_noise_steps', 3)
        cfg_scale = kwargs.get('cfg_scale', 1.0)
        sampler_name = kwargs.get('sampler_name', 'euler')
        scheduler = kwargs.get('scheduler', 'simple')
        frames = kwargs.get('frames', 65)
        fps = kwargs.get('fps', 16)
        
        # Advanced features
        prompt_assist = kwargs.get('prompt_assist', 'none')
        use_lightx2v = kwargs.get('use_lightx2v', True)
        lightx2v_rank = kwargs.get('lightx2v_rank', '32')
        lightx2v_strength = kwargs.get('lightx2v_strength', 3.0)
        use_pusa = kwargs.get('use_pusa', True)
        pusa_strength = kwargs.get('pusa_strength', 1.5)
        
        # LoRA settings
        use_lora = kwargs.get('use_lora', False)
        lora_url = kwargs.get('lora_url', None)
        lora_strength = kwargs.get('lora_strength', 1.0)
        civitai_token = kwargs.get('civitai_token', None)
        
        # Optimization settings
        use_sage_attention = kwargs.get('use_sage_attention', True)
        rel_l1_thresh = kwargs.get('rel_l1_thresh', 0.0)
        
        # Frame interpolation
        enable_interpolation = kwargs.get('enable_interpolation', False)
        interpolation_factor = kwargs.get('interpolation_factor', 2)
        
        # Generate seed
        if seed == 0:
            seed = random.randint(1, 2**32 - 1)
        
        logger.info(f"ğŸ¯ Configuration Summary:")
        logger.info(f"  ğŸ“ Resolution: {width}x{height} (mode: {aspect_mode}, auto720p: {auto_720p})")
        logger.info(f"  ğŸ¬ Animation: {frames} frames @ {fps}fps")
        logger.info(f"  âš™ï¸ Sampling: {steps} steps (high: {high_noise_steps}), CFG: {cfg_scale}")
        logger.info(f"  ğŸŒ± Seed: {seed}, Sampler: {sampler_name}")
        logger.info(f"  ğŸ­ Features: assist={prompt_assist}, lightx2v={lightx2v_rank}, pusa={use_pusa}")
        logger.info(f"  ğŸ”„ Interpolation: {enable_interpolation} (factor: {interpolation_factor})")
        
        if not COMFYUI_AVAILABLE:
            raise RuntimeError("ComfyUI not available")
        
        with torch.inference_mode():
            # Initialize nodes
            logger.info("ğŸ”§ Initializing ComfyUI nodes...")
            unet_loader = COMFYUI_NODES['UnetLoaderGGUF']()
            pathch_sage_attention = COMFYUI_NODES['PathchSageAttentionKJ']()
            wan_video_nag = COMFYUI_NODES['WanVideoNAG']()
            teacache = COMFYUI_NODES['WanVideoTeaCacheKJ']()
            model_sampling = COMFYUI_NODES['ModelSamplingSD3']()
            vae_loader = COMFYUI_NODES['VAELoader']()
            wan_image_to_video = COMFYUI_NODES['WanImageToVideo']()
            ksampler = COMFYUI_NODES['KSamplerAdvanced']()
            vae_decode = COMFYUI_NODES['VAEDecode']()
            
            # LoRA loaders
            prompt_assist_lora = COMFYUI_NODES['LoraLoaderModelOnly']()
            custom_lora = COMFYUI_NODES['LoraLoaderModelOnly']()
            lightx2v_lora = COMFYUI_NODES['LoraLoaderModelOnly']()
            pusa_lora = COMFYUI_NODES['LoraLoaderModelOnly']()
            
            logger.info("âœ… All nodes initialized")
            
            # Text encoding vá»›i CUDA safety
            logger.info("ğŸ“ Text encoding...")
            clip = safe_load_text_encoder()
            
            # Process prompts
            final_positive_prompt = positive_prompt
            if prompt_assist != "none":
                final_positive_prompt = f"{positive_prompt} {prompt_assist}."
                logger.info(f"ğŸ­ Enhanced prompt: {final_positive_prompt[:80]}...")
            
            positive = safe_encode_text(clip, final_positive_prompt)
            negative = safe_encode_text(clip, negative_prompt)
            
            del clip
            clear_memory_comprehensive()
            
            # Image processing vá»›i enhanced aspect control
            logger.info("ğŸ–¼ï¸ Enhanced image processing...")
            loaded_image, final_width, final_height = process_image_with_enhanced_aspect_control(
                image_path=image_path,
                target_width=width,
                target_height=height,
                aspect_mode=aspect_mode,
                auto_720p=auto_720p
            )
            
            # Update dimensions
            width, height = final_width, final_height
            
            # VAE loading
            logger.info("ğŸ¨ Loading VAE...")
            vae = vae_loader.load_vae("wan_2.1_vae.safetensors")[0]
            
            # Critical: Image to video encoding vá»›i comprehensive validation
            logger.info("ğŸ”„ Image to video encoding...")
            logger.info(f"ğŸ” Pre-encode validation:")
            logger.info(f"  Image: {loaded_image.shape} {loaded_image.dtype}")
            logger.info(f"  Target: {width}x{height}")
            logger.info(f"  Frames: {frames}")
            
            try:
                positive_out, negative_out, latent = wan_image_to_video.encode(
                    positive, negative, vae, width, height, frames, 1, loaded_image, None
                )
                logger.info("âœ… Image to video encoding successful")
            except Exception as e:
                logger.error(f"âŒ Image to video encoding failed: {e}")
                logger.error(f"  Image tensor: {loaded_image.shape} {loaded_image.dtype}")
                logger.error(f"  Parameters: {width}x{height}, {frames} frames")
                raise e
            
            # STAGE 1: High noise model
            logger.info("ğŸ¯ STAGE 1: High noise model...")
            model = unet_loader.load_unet("wan2.2_i2v_high_noise_14B_Q6_K.gguf")[0]
            
            # Apply prompt assist LoRA
            if prompt_assist != "none":
                lora_mapping = {
                    "walking to viewers": "walking to viewers_Wan.safetensors",
                    "walking from behind": "walking_from_behind.safetensors",
                    "b3ll13-d8nc3r": "b3ll13-d8nc3r.safetensors"
                }
                
                if prompt_assist in lora_mapping:
                    lora_file = lora_mapping[prompt_assist]
                    logger.info(f"ğŸ­ Applying prompt assist: {lora_file}")
                    model = prompt_assist_lora.load_lora_model_only(model, lora_file, 1.0)[0]
            
            # Apply custom LoRA
            if use_lora and lora_url:
                logger.info("ğŸ¨ Processing custom LoRA...")
                lora_path = download_lora_dynamic(lora_url, civitai_token)
                if lora_path:
                    model = custom_lora.load_lora_model_only(model, os.path.basename(lora_path), lora_strength)[0]
                    logger.info(f"âœ… Custom LoRA applied: {os.path.basename(lora_path)}")
            
            # Apply LightX2V LoRA
            if use_lightx2v:
                logger.info(f"âš¡ Applying LightX2V rank {lightx2v_rank}...")
                lightx2v_path = get_lightx2v_lora_path(lightx2v_rank)
                if lightx2v_path:
                    model = lightx2v_lora.load_lora_model_only(
                        model, os.path.basename(lightx2v_path), lightx2v_strength
                    )[0]
                    logger.info("âœ… LightX2V LoRA applied")
            
            # Apply optimizations
            if use_sage_attention:
                try:
                    model = pathch_sage_attention.patch(model, "auto")[0]
                    logger.info("âœ… Sage Attention applied")
                except Exception as e:
                    logger.warning(f"âš ï¸ Sage Attention failed: {e}")
            
            if rel_l1_thresh > 0:
                try:
                    model = teacache.patch_teacache(
                        model, rel_l1_thresh, 0.2, 1.0, "main_device", "14B"
                    )[0]
                    logger.info(f"âœ… TeaCache applied (threshold: {rel_l1_thresh})")
                except Exception as e:
                    logger.warning(f"âš ï¸ TeaCache failed: {e}")
            
            # High noise sampling
            logger.info(f"ğŸ¬ High noise sampling...")
            sampled = ksampler.sample(
                model=model,
                add_noise="enable",
                noise_seed=seed,
                steps=steps,
                cfg=cfg_scale,
                sampler_name=sampler_name,
                scheduler=scheduler,
                positive=positive_out,
                negative=negative_out,
                latent_image=latent,
                start_at_step=0,
                end_at_step=high_noise_steps,
                return_with_leftover_noise="enable"
            )[0]
            
            del model
            clear_memory_comprehensive()
            
            # STAGE 2: Low noise model
            logger.info("ğŸ¯ STAGE 2: Low noise model...")
            model = unet_loader.load_unet("wan2.2_i2v_low_noise_14B_Q6_K.gguf")[0]
            
            # Reapply LoRAs for low noise model
            if prompt_assist != "none" and prompt_assist in lora_mapping:
                model = prompt_assist_lora.load_lora_model_only(model, lora_mapping[prompt_assist], 1.0)[0]
            
            if use_lora and lora_url and lora_path:
                model = custom_lora.load_lora_model_only(model, os.path.basename(lora_path), lora_strength)[0]
            
            # Apply PUSA LoRA
            if use_pusa:
                logger.info(f"ğŸ­ Applying PUSA LoRA...")
                pusa_path = get_lightx2v_lora_path(lightx2v_rank)
                if pusa_path:
                    model = pusa_lora.load_lora_model_only(
                        model, os.path.basename(pusa_path), pusa_strength
                    )[0]
                    logger.info("âœ… PUSA LoRA applied")
            
            # Reapply optimizations
            if use_sage_attention:
                try:
                    model = pathch_sage_attention.patch(model, "auto")[0]
                except Exception as e:
                    logger.warning(f"âš ï¸ Sage Attention (stage 2) failed: {e}")
            
            if rel_l1_thresh > 0:
                try:
                    model = teacache.patch_teacache(
                        model, rel_l1_thresh, 0.2, 1.0, "main_device", "14B"
                    )[0]
                except Exception as e:
                    logger.warning(f"âš ï¸ TeaCache (stage 2) failed: {e}")
            
            # Low noise sampling
            logger.info(f"ğŸ¬ Low noise sampling...")
            sampled = ksampler.sample(
                model=model,
                add_noise="disable",
                noise_seed=seed,
                steps=steps,
                cfg=cfg_scale,
                sampler_name=sampler_name,
                scheduler=scheduler,
                positive=positive_out,
                negative=negative_out,
                latent_image=sampled,
                start_at_step=high_noise_steps,
                end_at_step=10000,
                return_with_leftover_noise="disable"
            )[0]
            
            del model
            clear_memory_comprehensive()
            
            # Decode frames
            logger.info("ğŸ¨ Decoding latents to frames...")
            decoded = vae_decode.decode(vae, sampled)[0]
            
            del vae
            clear_memory_comprehensive()
            
            # Save video
            logger.info("ğŸ’¾ Saving video vá»›i production method...")
            output_path = f"/app/ComfyUI/output/wan22_production_final_{uuid.uuid4().hex[:8]}.mp4"
            
            final_output_path = save_video_production_final(decoded, output_path, fps)
            
            # Frame interpolation (optional)
            if enable_interpolation and interpolation_factor > 1:
                logger.info(f"ğŸ”„ PRODUCTION FINAL RIFE interpolation...")
                
                pre_size = os.path.getsize(final_output_path) / (1024 * 1024)
                logger.info(f"ğŸ“Š Pre-interpolation: {pre_size:.1f}MB")
                
                interpolated_path = apply_rife_interpolation_production_final(
                    final_output_path, interpolation_factor
                )
                
                if interpolated_path != final_output_path and os.path.exists(interpolated_path):
                    post_size = os.path.getsize(interpolated_path) / (1024 * 1024)
                    logger.info(f"ğŸ“Š Post-interpolation: {post_size:.1f}MB")
                    logger.info(f"âœ… Frame interpolation successful!")
                    return interpolated_path
                else:
                    logger.warning("âš ï¸ Frame interpolation failed, using original")
                    return final_output_path
            
            return final_output_path
            
    except Exception as e:
        logger.error(f"âŒ Video generation failed: {e}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        return None
    finally:
        clear_memory_comprehensive()

# ================== STORAGE & VALIDATION ==================

def upload_to_minio_enhanced(local_path: str, object_name: str) -> str:
    """ğŸ“¤ Enhanced MinIO upload vá»›i comprehensive error handling"""
    try:
        if not minio_client:
            raise RuntimeError("MinIO client not initialized")
        
        if not os.path.exists(local_path):
            raise FileNotFoundError(f"Local file not found: {local_path}")
        
        file_size_mb = os.path.getsize(local_path) / (1024 * 1024)
        logger.info(f"ğŸ“¤ Uploading: {object_name} ({file_size_mb:.1f}MB)")
        
        # Upload vá»›i progress monitoring
        start_time = time.time()
        minio_client.fput_object(MINIO_BUCKET, object_name, local_path)
        upload_time = time.time() - start_time
        
        # Generate URL
        file_url = f"https://{MINIO_ENDPOINT}/{MINIO_BUCKET}/{quote(object_name)}"
        
        logger.info(f"âœ… Upload completed: {file_url}")
        logger.info(f"ğŸ“Š Upload stats: {file_size_mb:.1f}MB in {upload_time:.1f}s")
        
        return file_url
        
    except Exception as e:
        logger.error(f"âŒ Upload failed: {e}")
        raise e

def validate_input_comprehensive(job_input: dict) -> tuple[bool, str]:
    """ğŸ” PRODUCTION: Comprehensive input validation"""
    try:
        # Required parameters
        required_params = ["image_url", "positive_prompt"]
        for param in required_params:
            if param not in job_input or not job_input[param]:
                return False, f"Missing required parameter: {param}"
        
        # Image URL validation
        image_url = job_input["image_url"]
        try:
            response = requests.head(image_url, timeout=15)
            if response.status_code not in [200, 301, 302, 404]:  # 404 might still work
                logger.warning(f"âš ï¸ Image URL returned: {response.status_code}")
        except Exception as e:
            logger.warning(f"âš ï¸ Image URL validation warning: {e}")
        
        # Validate dimensions (reasonable limits)
        width = job_input.get("width", 720)
        height = job_input.get("height", 1280)
        if not (256 <= width <= 1536 and 256 <= height <= 1536):
            return False, "Width and height must be between 256 and 1536"
        
        # Validate frames (reasonable limit)
        frames = job_input.get("frames", 65)
        if not (1 <= frames <= 150):
            return False, "Frames must be between 1 and 150"
        
        # Enhanced prompt_assist validation
        prompt_assist = job_input.get("prompt_assist", "none")
        if isinstance(prompt_assist, str):
            prompt_assist_normalized = prompt_assist.lower().strip()
        else:
            prompt_assist_normalized = str(prompt_assist).lower().strip()
        
        # Comprehensive alias mapping
        valid_assists_map = {
            # Standard values
            "none": "none",
            "walking to viewers": "walking to viewers",
            "walking from behind": "walking from behind",
            "b3ll13-d8nc3r": "b3ll13-d8nc3r",
            
            # Common aliases
            "walking_to_viewers": "walking to viewers",
            "walking_from_behind": "walking from behind",
            "walk_to_viewers": "walking to viewers",
            "walk_from_behind": "walking from behind",
            "walkingtoviewers": "walking to viewers",
            "walkingfrombehind": "walking from behind",
            
            # Dance aliases
            "dance": "b3ll13-d8nc3r",
            "dancing": "b3ll13-d8nc3r",
            "dancer": "b3ll13-d8nc3r",
            "belly_dance": "b3ll13-d8nc3r",
            "belly-dance": "b3ll13-d8nc3r",
            "bellydance": "b3ll13-d8nc3r",
            
            # Empty/null handling
            "": "none",
            "null": "none",
            "undefined": "none"
        }
        
        if prompt_assist_normalized not in valid_assists_map:
            valid_display = ["none", "walking to viewers", "walking from behind", "b3ll13-d8nc3r"]
            return False, f"prompt_assist must be one of: {', '.join(valid_display)}"
        
        # Update with normalized value
        job_input["prompt_assist"] = valid_assists_map[prompt_assist_normalized]
        
        # Validate aspect mode
        aspect_mode = job_input.get("aspect_mode", "preserve")
        if aspect_mode not in ["preserve", "crop", "stretch"]:
            return False, "aspect_mode must be one of: preserve, crop, stretch"
        
        # Validate LightX2V rank
        lightx2v_rank = job_input.get("lightx2v_rank", "32")
        if str(lightx2v_rank) not in ["32", "64", "128"]:
            return False, "lightx2v_rank must be one of: 32, 64, 128"
        job_input["lightx2v_rank"] = str(lightx2v_rank)  # Ensure string
        
        # Validate numerical ranges
        cfg_scale = job_input.get("cfg_scale", 1.0)
        if not (0.1 <= cfg_scale <= 15.0):
            return False, "cfg_scale must be between 0.1 and 15.0"
        
        steps = job_input.get("steps", 6)
        if not (1 <= steps <= 50):
            return False, "steps must be between 1 and 50"
        
        high_noise_steps = job_input.get("high_noise_steps", 3)
        if not (1 <= high_noise_steps < steps):
            return False, f"high_noise_steps must be between 1 and {steps-1}"
        
        # Validate sampler
        sampler_name = job_input.get("sampler_name", "euler")
        valid_samplers = [
            "euler", "euler_ancestral", "dpm_2", "dpm_2_ancestral", 
            "lms", "dpm_fast", "dpm_adaptive", "dpmpp_2s_ancestral", 
            "dpmpp_sde", "dpmpp_2m", "ddim"
        ]
        if sampler_name not in valid_samplers:
            return False, f"Invalid sampler. Must be one of: {', '.join(valid_samplers)}"
        
        # Validate interpolation
        interpolation_factor = job_input.get("interpolation_factor", 2)
        if not (2 <= interpolation_factor <= 8):
            return False, "interpolation_factor must be between 2 and 8"
        
        return True, "All parameters valid"
        
    except Exception as e:
        logger.error(f"âŒ Validation error: {e}")
        return False, f"Validation error: {str(e)}"

# ================== MAIN HANDLER (PRODUCTION FINAL) ==================

def handler(job):
    """
    ğŸš€ PRODUCTION FINAL: Main RunPod handler
    âœ… Comprehensive CUDA safety, tensor validation, aspect control, RIFE interpolation
    ğŸ¯ Supports all popular image sizes vÃ  production deployment
    """
    job_id = job.get("id", "unknown")
    start_time = time.time()
    
    try:
        job_input = job.get("input", {})
        
        # Comprehensive input validation
        is_valid, validation_message = validate_input_comprehensive(job_input)
        if not is_valid:
            return {
                "error": validation_message,
                "status": "failed",
                "job_id": job_id,
                "processing_time_seconds": round(time.time() - start_time, 2)
            }
        
        # System readiness checks
        if not COMFYUI_AVAILABLE:
            return {
                "error": "ComfyUI system not available",
                "status": "failed",
                "job_id": job_id,
                "system_issue": "comfyui_import_failed"
            }
        
        models_ok, missing_models = verify_models_comprehensive()
        if not models_ok:
            return {
                "error": "Required models missing",
                "missing_models": missing_models,
                "status": "failed",
                "job_id": job_id
            }
        
        # Extract validated parameters
        image_url = job_input["image_url"]
        positive_prompt = job_input["positive_prompt"]
        
        logger.info(f"ğŸš€ Job {job_id}: WAN2.2 PRODUCTION FINAL started")
        logger.info(f"ğŸ–¼ï¸ Image: {image_url}")
        logger.info(f"ğŸ“ Prompt: {positive_prompt[:100]}...")
        logger.info(f"âš™ï¸ System: CUDA={CUDA_AVAILABLE}, ComfyUI={COMFYUI_AVAILABLE}")
        
        with tempfile.TemporaryDirectory() as temp_dir:
            # Download input image
            image_path = os.path.join(temp_dir, "input_image.jpg")
            
            try:
                logger.info("ğŸ“¥ Downloading input image...")
                response = requests.get(image_url, timeout=60, stream=True)
                response.raise_for_status()
                
                with open(image_path, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
                
                image_size_mb = os.path.getsize(image_path) / (1024 * 1024)
                logger.info(f"âœ… Image downloaded: {image_size_mb:.1f}MB")
                
                # Basic image validation
                try:
                    with Image.open(image_path) as img:
                        img_width, img_height = img.size
                        logger.info(f"ğŸ“Š Image dimensions: {img_width}x{img_height}")
                        
                        if img_width < 64 or img_height < 64:
                            return {"error": "Image too small (minimum 64x64)"}
                        if img_width > 4096 or img_height > 4096:
                            return {"error": "Image too large (maximum 4096x4096)"}
                except Exception as e:
                    logger.warning(f"âš ï¸ Image validation warning: {e}")
                
            except Exception as e:
                return {
                    "error": f"Failed to download image: {str(e)}",
                    "status": "failed",
                    "job_id": job_id
                }
            
            # Generate video vá»›i PRODUCTION FINAL method
            logger.info("ğŸ¬ Starting PRODUCTION FINAL video generation...")
            generation_start = time.time()
            
            output_path = generate_video_wan22_production_final(
                image_path=image_path,
                **job_input
            )
            
            generation_time = time.time() - generation_start
            
            if not output_path or not os.path.exists(output_path):
                return {
                    "error": "Video generation failed - no output produced",
                    "status": "failed",
                    "job_id": job_id
                }
            
            # Upload result vá»›i enhanced method
            logger.info("ğŸ“¤ Uploading result...")
            output_filename = f"wan22_production_final_{job_id}_{uuid.uuid4().hex[:8]}.mp4"
            
            try:
                output_url = upload_to_minio_enhanced(output_path, output_filename)
            except Exception as e:
                return {
                    "error": f"Failed to upload result: {str(e)}",
                    "status": "failed",
                    "job_id": job_id
                }
            
            # Calculate comprehensive statistics
            total_time = time.time() - start_time
            file_size_mb = os.path.getsize(output_path) / (1024 * 1024)
            duration_seconds = job_input.get("frames", 65) / job_input.get("fps", 16)
            
            # Check if interpolated
            is_interpolated = (job_input.get("enable_interpolation", False) and 
                             "_rife_" in os.path.basename(output_path))
            actual_interpolation_factor = (job_input.get("interpolation_factor", 2) 
                                         if is_interpolated else 1)
            
            actual_seed = job_input.get("seed", 0) if job_input.get("seed", 0) != 0 else "auto-generated"
            
            logger.info(f"âœ… Job {job_id} COMPLETED SUCCESSFULLY!")
            logger.info(f"â±ï¸ Total: {total_time:.1f}s (generation: {generation_time:.1f}s)")
            logger.info(f"ğŸ“Š Output: {file_size_mb:.1f}MB, duration: {duration_seconds:.1f}s")
            logger.info(f"ğŸ¬ Features used: aspect={job_input.get('aspect_mode', 'preserve')}, interpolation={is_interpolated}")
            
            return {
                "output_video_url": output_url,
                "processing_time_seconds": round(total_time, 2),
                "generation_time_seconds": round(generation_time, 2),
                
                "video_info": {
                    "width": job_input.get("width", 720),
                    "height": job_input.get("height", 1280),
                    "frames": job_input.get("frames", 65),
                    "fps": job_input.get("fps", 16),
                    "duration_seconds": round(duration_seconds, 2),
                    "file_size_mb": round(file_size_mb, 2),
                    "interpolated": is_interpolated,
                    "interpolation_factor": actual_interpolation_factor,
                    "aspect_mode": job_input.get("aspect_mode", "preserve"),
                    "auto_720p": job_input.get("auto_720p", False)
                },
                
                "generation_params": {
                    "positive_prompt": job_input["positive_prompt"],
                    "negative_prompt": (job_input.get("negative_prompt", "")[:80] + "..." 
                                      if len(job_input.get("negative_prompt", "")) > 80 
                                      else job_input.get("negative_prompt", "")),
                    "steps": job_input.get("steps", 6),
                    "high_noise_steps": job_input.get("high_noise_steps", 3),
                    "cfg_scale": job_input.get("cfg_scale", 1.0),
                    "seed": actual_seed,
                    "sampler_name": job_input.get("sampler_name", "euler"),
                    "scheduler": job_input.get("scheduler", "simple"),
                    "prompt_assist": job_input.get("prompt_assist", "none"),
                    
                    "lightx2v_config": {
                        "enabled": job_input.get("use_lightx2v", True),
                        "rank": job_input.get("lightx2v_rank", "32"),
                        "strength": job_input.get("lightx2v_strength", 3.0)
                    },
                    
                    "pusa_config": {
                        "enabled": job_input.get("use_pusa", True),
                        "strength": job_input.get("pusa_strength", 1.5)
                    },
                    
                    "lora_config": {
                        "enabled": job_input.get("use_lora", False),
                        "url": job_input.get("lora_url", None),
                        "strength": job_input.get("lora_strength", 1.0)
                    },
                    
                    "aspect_control": {
                        "mode": job_input.get("aspect_mode", "preserve"),
                        "auto_720p": job_input.get("auto_720p", False)
                    },
                    
                    "interpolation": {
                        "enabled": job_input.get("enable_interpolation", False),
                        "factor": job_input.get("interpolation_factor", 2)
                    },
                    
                    "optimizations": {
                        "sage_attention": job_input.get("use_sage_attention", True),
                        "teacache_threshold": job_input.get("rel_l1_thresh", 0.0)
                    },
                    
                    "system_info": {
                        "cuda_available": CUDA_AVAILABLE,
                        "comfyui_available": COMFYUI_AVAILABLE,
                        "model_quantization": "Q6_K",
                        "workflow_version": "PRODUCTION_FINAL_v4.0",
                        "tensor_fixes_applied": True,
                        "aspect_control_enhanced": True,
                        "rife_interpolation_fixed": True
                    }
                },
                
                "status": "completed"
            }
            
    except Exception as e:
        error_msg = str(e)
        logger.error(f"âŒ Handler error for job {job_id}: {error_msg}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        
        return {
            "error": f"Generation failed: {error_msg}",
            "status": "failed",
            "job_id": job_id,
            "processing_time_seconds": round(time.time() - start_time, 2),
            "system_info": {
                "cuda_available": CUDA_AVAILABLE,
                "comfyui_available": COMFYUI_AVAILABLE,
                "error_category": "generation_error"
            }
        }
        
    finally:
        clear_memory_comprehensive()

# ================== HEALTH CHECK (PRODUCTION FINAL) ==================

def health_check():
    """ğŸ¥ PRODUCTION FINAL: Comprehensive system health check"""
    try:
        issues = []
        warnings = []
        
        # CUDA check
        if not torch.cuda.is_available():
            issues.append("CUDA not available")
        elif not CUDA_AVAILABLE:
            warnings.append("CUDA compatibility issues detected")
        
        # GPU memory check
        if torch.cuda.is_available():
            try:
                memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9
                if memory_gb < 8:
                    warnings.append(f"Low GPU memory: {memory_gb:.1f}GB")
                elif memory_gb < 16:
                    warnings.append(f"Moderate GPU memory: {memory_gb:.1f}GB")
                
                # Test basic GPU operations
                test_tensor = torch.ones(100, device='cuda')
                test_result = test_tensor.sum()
                del test_tensor, test_result
                torch.cuda.empty_cache()
                
            except Exception as e:
                issues.append(f"GPU test failed: {str(e)}")
        
        # ComfyUI check
        if not COMFYUI_AVAILABLE:
            issues.append("ComfyUI modules not available")
        
        # Models check
        models_ok, missing = verify_models_comprehensive()
        if not models_ok:
            issues.append(f"Missing {len(missing)} required models")
        
        # Storage check
        if not minio_client:
            issues.append("MinIO storage not available")
        else:
            try:
                # Test MinIO connectivity
                buckets = minio_client.list_buckets()
                logger.debug(f"MinIO buckets accessible: {len(buckets)}")
            except Exception as e:
                warnings.append(f"MinIO connectivity issue: {str(e)}")
        
        # Disk space check
        try:
            import shutil
            total, used, free = shutil.disk_usage("/app")
            free_gb = free / (1024**3)
            if free_gb < 5:
                issues.append(f"Low disk space: {free_gb:.1f}GB free")
            elif free_gb < 10:
                warnings.append(f"Moderate disk space: {free_gb:.1f}GB free")
        except Exception as e:
            warnings.append(f"Disk space check failed: {str(e)}")
        
        # RIFE check
        rife_script = "/app/Practical-RIFE/inference_video.py"
        if not os.path.exists(rife_script):
            warnings.append("RIFE interpolation not available")
        
        # Summary
        if issues:
            return False, f"Critical issues: {'; '.join(issues)}" + (f" | Warnings: {'; '.join(warnings)}" if warnings else "")
        elif warnings:
            return True, f"Operational with warnings: {'; '.join(warnings)}"
        else:
            return True, "All systems operational - PRODUCTION READY"
            
    except Exception as e:
        return False, f"Health check failed: {str(e)}"

# ================== STARTUP DIAGNOSTICS ==================

def startup_diagnostics():
    """ğŸ” Comprehensive startup diagnostics"""
    logger.info("ğŸ” Running startup diagnostics...")
    
    # System info
    logger.info(f"ğŸ”¥ PyTorch: {torch.__version__}")
    logger.info(f"ğŸ Python: {sys.version.split()[0]}")
    
    if torch.cuda.is_available():
        device_count = torch.cuda.device_count()
        for i in range(device_count):
            props = torch.cuda.get_device_properties(i)
            memory_gb = props.total_memory / 1e9
            logger.info(f"ğŸ¯ GPU {i}: {props.name}, {memory_gb:.1f}GB, Compute {props.major}.{props.minor}")
    else:
        logger.warning("âš ï¸ No CUDA devices available")
    
    # Memory info
    try:
        import psutil
        memory = psutil.virtual_memory()
        logger.info(f"ğŸ’¾ RAM: {memory.total / 1e9:.1f}GB total, {memory.available / 1e9:.1f}GB available")
    except ImportError:
        logger.info("ğŸ’¾ RAM info not available (psutil not installed)")
    
    # Disk space
    try:
        import shutil
        total, used, free = shutil.disk_usage("/app")
        logger.info(f"ğŸ’¿ Disk: {total / 1e9:.1f}GB total, {free / 1e9:.1f}GB free")
    except Exception as e:
        logger.warning(f"âš ï¸ Disk info failed: {e}")
    
    # Feature summary
    features = []
    if CUDA_AVAILABLE:
        features.append("CUDA")
    if COMFYUI_AVAILABLE:
        features.append("ComfyUI")
    if minio_client:
        features.append("MinIO")
    if os.path.exists("/app/Practical-RIFE/inference_video.py"):
        features.append("RIFE")
    
    logger.info(f"âœ¨ Available features: {', '.join(features) if features else 'None'}")

# ================== MAIN ENTRY POINT (PRODUCTION FINAL) ==================

if __name__ == "__main__":
    logger.info("ğŸš€ Starting WAN2.2 LightX2V PRODUCTION FINAL Serverless Worker...")
    
    try:
        # Comprehensive startup diagnostics
        startup_diagnostics()
        
        # Run health check
        health_ok, health_msg = health_check()
        
        logger.info(f"ğŸ“Š System Health: {health_msg}")
        
        if not health_ok:
            logger.error("âŒ Critical health check failures detected!")
            logger.error("ğŸ’¥ System cannot start in production mode!")
            
            # Try to continue with limited functionality
            logger.info("ğŸ”„ Attempting to start with limited functionality...")
        
        # Feature availability summary
        logger.info("ğŸ“‹ PRODUCTION FINAL Features:")
        logger.info("  âœ… Enhanced CUDA Compatibility Fixes")
        logger.info("  âœ… Comprehensive Tensor Shape Validation")
        logger.info("  âœ… Advanced Aspect Ratio Control (preserve/crop/stretch)")
        logger.info("  âœ… Multi-Size Image Support (64x64 to 4096x4096)")
        logger.info("  âœ… RIFE Frame Interpolation (FIXED)")
        logger.info("  âœ… LightX2V LoRA Support (rank 32/64/128)")
        logger.info("  âœ… Custom LoRA Download (HuggingFace/CivitAI)")
        logger.info("  âœ… Advanced Optimizations (Sage Attention, TeaCache)")
        logger.info("  âœ… Production-Grade Error Handling")
        logger.info("  âœ… Comprehensive Input Validation")
        logger.info("  âœ… Enhanced Storage Management")
        logger.info("  âœ… Detailed Performance Monitoring")
        
        # System readiness status
        readiness_indicators = {
            "CUDA": "âœ…" if CUDA_AVAILABLE else "âŒ",
            "ComfyUI": "âœ…" if COMFYUI_AVAILABLE else "âŒ",
            "Storage": "âœ…" if minio_client else "âŒ",
            "Models": "âœ…" if verify_models_comprehensive()[0] else "âŒ",
            "RIFE": "âœ…" if os.path.exists("/app/Practical-RIFE/inference_video.py") else "âš ï¸"
        }
        
        readiness_summary = " | ".join([f"{k}: {v}" for k, v in readiness_indicators.items()])
        logger.info(f"ğŸ¯ System Readiness: {readiness_summary}")
        
        if all(v == "âœ…" for k, v in readiness_indicators.items() if k != "RIFE"):
            logger.info("ğŸ¬ PRODUCTION FINAL READY - All critical systems operational!")
        elif COMFYUI_AVAILABLE and verify_models_comprehensive()[0]:
            logger.info("ğŸ¬ BASIC READY - Core functionality available!")
        else:
            logger.warning("âš ï¸ LIMITED FUNCTIONALITY - Some systems unavailable!")
        
        # Performance tips
        if CUDA_AVAILABLE:
            memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9
            if memory_gb >= 24:
                logger.info("ğŸš€ HIGH PERFORMANCE MODE - Optimal GPU memory available")
            elif memory_gb >= 16:
                logger.info("âš¡ PERFORMANCE MODE - Good GPU memory available")
            elif memory_gb >= 8:
                logger.info("ğŸ”§ EFFICIENCY MODE - Limited GPU memory, using optimizations")
            else:
                logger.warning("âš ï¸ LOW MEMORY MODE - Consider enabling aggressive optimizations")
        
        # Final startup message
        logger.info("=" * 80)
        logger.info("ğŸ‰ WAN2.2 PRODUCTION FINAL Worker Started Successfully!")
        logger.info("ğŸ“ Ready to process video generation requests...")
        logger.info("ğŸ”— RunPod integration active")
        logger.info("=" * 80)
        
        # Start RunPod serverless worker
        runpod.serverless.start({"handler": handler})
        
    except KeyboardInterrupt:
        logger.info("ğŸ›‘ Shutdown requested by user")
        sys.exit(0)
        
    except Exception as e:
        logger.error(f"âŒ Critical startup failure: {e}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        
        logger.error("ğŸ’¥ SYSTEM CANNOT START")
        logger.error("ğŸ”§ Please check:")
        logger.error("  - CUDA drivers and compatibility")
        logger.error("  - ComfyUI installation and dependencies")
        logger.error("  - Model files availability")
        logger.error("  - Storage connectivity")
        logger.error("  - System resources (memory, disk space)")
        
        sys.exit(1)
